{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hubs/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "from pydub import AudioSegment\n",
    "from scipy.io import wavfile\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"data/audio_path.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    transcriptions = json.load(f)\n",
    "\n",
    "labels = np.array([transcription[\"transcription\"] for transcription in transcriptions])\n",
    "data = labels\n",
    "\n",
    "vocabularies = []\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "for sentence in data:\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token not in word_to_index:\n",
    "            word_to_index[token] = len(word_to_index)\n",
    "            index_to_word[len(index_to_word)] = token\n",
    "            vocabularies.append(token)\n",
    "\n",
    "# Encodage des séquences\n",
    "encoded_data = []\n",
    "for sentence in data:\n",
    "    tokens = sentence.split()\n",
    "    encoded_sentence = [word_to_index[token] for token in tokens]\n",
    "    encoded_data.append(encoded_sentence)\n",
    "\n",
    "\n",
    "# Création du Dataset et DataLoader\n",
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index])\n",
    "\n",
    "\n",
    "dataset = LanguageDataset(encoded_data)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "with open(\"data/labels.json\", \"w\") as f:\n",
    "        json.dump(vocabularies, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'height', 'nine']\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/labels.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction de label d'une nouvelle audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mean_length(file_src=\"data/mean_length.json\"):\n",
    "    if not os.path.exists(file_src):\n",
    "        print(\n",
    "            \"Le fichier contenant la longueur moyenne n'existe pas. Cela peut affecter la pprecision du modele\"\n",
    "        )\n",
    "        return 0.4374343333333333\n",
    "\n",
    "    with open(file_src, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "class Custom_preprocessing:\n",
    "    def __init__(self, audio_path, output_file):\n",
    "        self.audio = audio_path\n",
    "        # Charger les fichiers audio et calculer leurs longueurs\n",
    "        self.output_file = output_file\n",
    "\n",
    "        self.target_length = load_mean_length()\n",
    "        self.process()\n",
    "\n",
    "    # Élimination des silences\n",
    "    def remove_silence(self, audio_path, output):\n",
    "        # Charger le fichier audio\n",
    "        signal, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "        # Détection des régions actives dans le signal\n",
    "        non_silent_intervals = librosa.effects.split(signal, top_db=30)\n",
    "\n",
    "        # Fusionner les intervalles non silencieux\n",
    "        non_silent_signal = librosa.effects.remix(signal, non_silent_intervals)\n",
    "\n",
    "        # Sauvegarder le signal audio sans les silences\n",
    "        sf.write(output, non_silent_signal, sr)\n",
    "\n",
    "    # Normalisation du volume\n",
    "    def normalize_audio_volume(self, audio_path, output_path, target_dBFS=-20.0):\n",
    "        # Chargement de l'enregistrement audio\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Calcul du facteur de normalisation pour atteindre le niveau cible\n",
    "        current_dBFS = audio.dBFS\n",
    "        normalization_factor = target_dBFS - current_dBFS\n",
    "\n",
    "        # Normalisation du volume de l'audio\n",
    "        normalized_audio = audio + normalization_factor\n",
    "\n",
    "        # Export de l'audio normalisé\n",
    "        normalized_audio.export(output_path, format=\"wav\")\n",
    "\n",
    "    # Filtrage du bruit\n",
    "    def filtrage_du_bruit(self, audio_path, output, noise_threshold=-40.0):\n",
    "        # Chargement de l'enregistrement audio\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Détection du bruit de fond\n",
    "        background_noise = audio.dBFS\n",
    "\n",
    "        # Filtrer le bruit de fond\n",
    "        if background_noise > noise_threshold:\n",
    "            audio = audio - noise_threshold\n",
    "        else:\n",
    "            audio = audio - background_noise\n",
    "\n",
    "        # Export de l'audio filtré\n",
    "        audio.export(output, format=\"wav\")\n",
    "\n",
    "    # Segmentation de la parole\n",
    "    def segmentation_parole(self, audio_path, output_file, silence_threshold=-45):\n",
    "        # Charger le fichier audio\n",
    "        audio = AudioSegment.from_file(audio_path, format=\"wav\")\n",
    "\n",
    "        # Détection des silences\n",
    "        non_silent_audio = audio.strip_silence(silence_thresh=silence_threshold)\n",
    "\n",
    "        # Exporter le fichier audio sans les silences\n",
    "        non_silent_audio.export(output_file, format=\"wav\")\n",
    "\n",
    "    # Éliminé des artefacts\n",
    "    def remove_artifacts(self, audio_path, output_path):\n",
    "        # Chargement de l'enregistrement audio\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Suppression d'artefacts basée sur la fréquence ou l'amplitude\n",
    "        # Par exemple, supprimer les fréquences inférieures à 1000 Hz\n",
    "        audio_filtered = audio.low_pass_filter(1000)\n",
    "\n",
    "        # Export de l'audio filtré\n",
    "        audio_filtered.export(output_path, format=\"wav\")\n",
    "\n",
    "    # Préaccentuation\n",
    "    def preaccentuation(self, audio_file, output):\n",
    "        # Charger le signal vocal (remplacer \"audio.wav\" par votre propre fichier audio)\n",
    "        sample_rate, audio_data = wavfile.read(audio_file)\n",
    "\n",
    "        # Paramètres de la préaccentuation\n",
    "        alpha = 0.95  # Facteur de préaccentuation (typiquement entre 0.9 et 1)\n",
    "\n",
    "        # Appliquer la préaccentuation\n",
    "        preemphasis_audio = np.append(\n",
    "            audio_data[0], audio_data[1:] - alpha * audio_data[:-1]\n",
    "        )\n",
    "\n",
    "        # Enregistrer le signal filtré en tant que fichier WAV\n",
    "        wavfile.write(output, sample_rate, np.int16(preemphasis_audio))\n",
    "\n",
    "    # Normalisation temporelle\n",
    "    def time_stretch_audio(self, input_file, output_file, target_duration):\n",
    "        # Charger l'audio\n",
    "        audio, sr = librosa.load(input_file)\n",
    "\n",
    "        # Calculer la durée actuelle\n",
    "        current_duration = len(audio) / 44100\n",
    "\n",
    "        # Calculer le facteur de normalisation\n",
    "        speed_factor = current_duration / target_duration\n",
    "\n",
    "        # Normaliser l'audio en modifiant la vitesse\n",
    "        normalized_audio = librosa.effects.time_stretch(y=audio, rate=speed_factor)\n",
    "\n",
    "        # Sauvegarder l'audio normalisé\n",
    "        sf.write(output_file, normalized_audio, sr)\n",
    "\n",
    "    # Convertir en_format adapté\n",
    "    def convert_to_spectrogram(self, audio_path):\n",
    "        transform = Compose(\n",
    "            [\n",
    "                torchaudio.transforms.Resample(\n",
    "                    orig_freq=44100, new_freq=16000\n",
    "                ),  # Échantillonnage à 16 kHz\n",
    "                MelSpectrogram(\n",
    "                    n_fft=400, win_length=400, hop_length=160, n_mels=128\n",
    "                ),  # Créer un spectrogramme Mel\n",
    "            ]\n",
    "        )\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        spectrogram = transform(waveform).unsqueeze(0)  # Ajouter une dimension de lot\n",
    "        spectrogram = np.array(spectrogram)\n",
    "        spectrogram = spectrogram.reshape(-1)\n",
    "\n",
    "        return spectrogram, sample_rate\n",
    "\n",
    "    # Correction d'accent\n",
    "    def correction_accent(self, audio_file):\n",
    "        signal, sample_rate = librosa.load(audio_file, sr=None)\n",
    "\n",
    "        # Calculer les coefficients cepstraux MFCC\n",
    "        mfccs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=13)\n",
    "\n",
    "        # Normaliser les coefficients MFCC\n",
    "        scaler = StandardScaler()\n",
    "        return scaler.fit_transform(mfccs.T).T\n",
    "\n",
    "    def process(self):\n",
    "        mfcc_features_list = []\n",
    "        self.remove_silence(self.audio, \"temp/remove_silence.wav\")\n",
    "        self.normalize_audio_volume(\"temp/remove_silence.wav\", \"temp/normalize_audio_volume.wav\")\n",
    "        self.time_stretch_audio(\n",
    "            \"temp/normalize_audio_volume.wav\", \"temp/time_stretch_audio.wav\", self.target_length\n",
    "        )\n",
    "        self.filtrage_du_bruit(\"temp/time_stretch_audio.wav\", \"temp/filtrage_du_bruit.wav\")\n",
    "        self.segmentation_parole(\"temp/filtrage_du_bruit.wav\", \"temp/segmentation_parole.wav\")\n",
    "        self.remove_artifacts(\"temp/segmentation_parole.wav\", \"temp/remove_artifacts.wav\")\n",
    "        self.preaccentuation(\"temp/remove_artifacts.wav\", \"temp/preaccentuation.wav\")\n",
    "        mfcc_features_normalized = self.correction_accent(\"temp/preaccentuation.wav\")\n",
    "        mfcc_features_list.append(mfcc_features_normalized)\n",
    "\n",
    "        X = np.array(mfcc_features_list)\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "        # Enregistrement des caractéristiques prétraitées dans un fichier (optionnel)\n",
    "        np.save(self.output_file, X)\n",
    "\n",
    "\n",
    "audio_ = \"./data/audio_chiffre/7_theo_43.wav\"\n",
    "test = Custom_preprocessing(audio_, \"data/works.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Obtenir le chemin du répertoire temporaire\n",
    "temp_dir = \"temp/\"\n",
    "\n",
    "# Parcourir les fichiers du répertoire temporaire\n",
    "for file in os.listdir(temp_dir):\n",
    "    # Vérifier si c'est un fichier\n",
    "    if os.path.isfile(os.path.join(temp_dir, file)):\n",
    "        # Supprimer le fichier\n",
    "        os.remove(os.path.join(temp_dir, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 494)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SpeechRecognition' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m X_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(X)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Charger le modèle entraîné\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSpeechRecognition\u001b[49m(hidden_size\u001b[38;5;241m=\u001b[39mhidden_size, num_classes\u001b[38;5;241m=\u001b[39mnum_classes, n_feats\u001b[38;5;241m=\u001b[39minput_size, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SpeechRecognition' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.load(\"works.npy\")\n",
    "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "print(X.shape)\n",
    "X_tensor = torch.Tensor(X)\n",
    "# Charger le modèle entraîné\n",
    "model = SpeechRecognition(hidden_size=hidden_size, num_classes=num_classes, n_feats=input_size, num_layers=1, dropout=0.1)\n",
    "model.load_state_dict(torch.load('data/model.pth'))\n",
    "model.eval()\n",
    "\n",
    "def decode_output(output, vocab):\n",
    "    decoded_sequence = []\n",
    "    print(output.shape)\n",
    "    for timestep_output in output:\n",
    "        predicted_symbol_index = torch.argmax(timestep_output).item()\n",
    "        predicted_symbol = vocab[predicted_symbol_index]\n",
    "        decoded_sequence.append(predicted_symbol)\n",
    "    return ' '.join(decoded_sequence)\n",
    "\n",
    "# Utiliser le modèle pour transcrire l'audio\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs, _ = model(X_tensor.unsqueeze(0), model._init_hidden(X_tensor.size(0)))\n",
    "    decoded_text = decode_output(outputs, vocab)\n",
    "    print(\"Texte transcrit :\", decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
