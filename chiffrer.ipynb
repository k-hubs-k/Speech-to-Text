{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pydub.utils import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wakeword model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWakeWord(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, feature_size, hidden_size,\n",
    "                num_layers, dropout, bidirectional, device='cpu'):\n",
    "        super(LSTMWakeWord, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "        self.device = device\n",
    "        self.layernorm = nn.LayerNorm(feature_size)\n",
    "        self.lstm = nn.LSTM(input_size=feature_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, dropout=dropout,\n",
    "                            bidirectional=bidirectional)\n",
    "        self.classifier = nn.Linear(hidden_size*self.directions, num_classes)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        n, d, hs = self.num_layers, self.directions, self.hidden_size\n",
    "        return (torch.zeros(n*d, batch_size, hs).to(self.device),\n",
    "                torch.zeros(n*d, batch_size, hs).to(self.device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape => seq_len, batch, feature\n",
    "        x = self.layernorm(x)\n",
    "        hidden = self._init_hidden(x.size()[1])\n",
    "        out, (hn, cn) = self.lstm(x, hidden)\n",
    "        out = self.classifier(hn)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        # outputs: (batch_size, seq_len, hidden_size)\n",
    "        # hidden: (num_layers, batch_size, hidden_size)\n",
    "        # cell: (num_layers, batch_size, hidden_size)\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input: (batch_size)\n",
    "        input = input.unsqueeze(1)  # (batch_size, 1)\n",
    "        embedded = self.embedding(input)  # (batch_size, 1, hidden_size)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))  # (batch_size, output_size)\n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation des DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser X et y en un ensemble d'entraînement (80%) et un ensemble de validation (20%)\n",
    "X = np.load(\"mfcc_feature1.npy\")\n",
    "y = np.load(\"labels.npy\")\n",
    "\n",
    "X_train, X_set, y_train, y_set = train_test_split(X, y, test_size=0.2, random_state=34)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_set, y_set, test_size=0.5, random_state=34)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], 1, X_valid.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "# Définir les hyperparamètres\n",
    "input_size = X_train.shape[2]  # Nombre de coefficients MFCC\n",
    "hidden_size = 1024  # Taille de la couche cachée\n",
    "num_classes = y.shape[0]  # Nombre de classes (mots/phrases)\n",
    "num_epochs = 120\n",
    "batch_size = len(X_train)\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convertir les données en tensors PyTorch\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "X_valid_tensor = torch.Tensor(X_valid)\n",
    "X_test_tensor = torch.Tensor(X_test)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "y_valid_tensor = torch.LongTensor(y_valid)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Créer des DataLoader PyTorch\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=len(X_valid), shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(X_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enregistrement des hyper parametres du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = {\n",
    "    \"input_size\": input_size,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learning_rate\n",
    "}\n",
    "\n",
    "with open(\"dataset/model_parameters.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(model_parameters, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_size': 494, 'hidden_size': 1024, 'num_classes': 3000, 'num_epochs': 120, 'batch_size': 2400, 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset/model_parameters.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model LSTM + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActDropNormRNN(nn.Module):\n",
    "    def __init__(self, n_feats, dropout, keep_shape=False):\n",
    "        super(ActDropNormRNN, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(n_feats)\n",
    "        self.keep_shape = keep_shape\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        # x = self.norm(self.dropout(F.gelu(x)))\n",
    "        x = self.dropout(F.gelu(self.norm(x)))\n",
    "        if self.keep_shape:\n",
    "            return x.transpose(1, 2)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class SpeechRecognition(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, num_classes, n_feats, num_layers, dropout):\n",
    "        super(SpeechRecognition, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = ModelRNN(n_feats, hidden_size)  # Utilisation du RNN personnalisé\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.LayerNorm(input_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.LayerNorm(input_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, dropout=0.0,\n",
    "                            bidirectional=False)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.final_fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        n, hs = self.num_layers, self.hidden_size\n",
    "        return (torch.zeros(n*1, batch_size, hs),\n",
    "                torch.zeros(n*1, batch_size, hs))\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = x.squeeze(1)  # batch, feature, time\n",
    "        x = self.rnn(x)  # batch, time, hidden_size\n",
    "        x = self.dense(x) # batch, time, feature\n",
    "        x = x.transpose(0, 1) # time, batch, feature\n",
    "        out, (hn, cn) = self.lstm(x, hidden)\n",
    "        # Ajustement des dimensions de hx et cx\n",
    "        hn = hn.squeeze(0)  # hn devient 2D\n",
    "        cn = cn.squeeze(0)  # cn devient 2D\n",
    "        x = self.dropout2(F.gelu(self.layer_norm2(out)))  # (time, batch, n_class)\n",
    "        return self.final_fc(x), (hn, cn)\n",
    "\n",
    "class ModelRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ModelRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return out\n",
    "\n",
    "class ModelLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):\n",
    "        super(ModelLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, (hn, cn) = self.lstm(x, hidden)\n",
    "        x = self.dropout(F.gelu(self.layer_norm(out)))  # Shape: batch_size, time_steps, hidden_size\n",
    "        return self.final_fc(x), (hn, cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction pour calculer la précision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs, _ = model(inputs.unsqueeze(1), model._init_hidden(inputs.size(0)))\n",
    "            _, predicted = torch.max(outputs.squeeze(0).data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece3d635",
   "metadata": {},
   "source": [
    "# entraînement du modèle de classification vocale en utilisant PyTorch et les caractéristiques MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hubs/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "  0%|          | 0/120 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m model(inputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), model\u001b[38;5;241m.\u001b[39m_init_hidden(inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)))  \u001b[38;5;66;03m# Ajout de unsqueeze(1) pour correspondre à la taille du batch\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels)  \u001b[38;5;66;03m# Ajustement pour correspondre à la taille du batch\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Définition du modèle\n",
    "model = SpeechRecognition(hidden_size=hidden_size, num_classes=num_classes, n_feats=input_size, num_layers=1, dropout=0.1)\n",
    "\n",
    "# Définition de la fonction de perte et de l'optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "best_train_acc = 0\n",
    "best_test_acc = 0\n",
    "# Entraînement du modèle\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs.unsqueeze(1), model._init_hidden(inputs.size(0)))  # Ajout de unsqueeze(1) pour correspondre à la taille du batch\n",
    "        loss = criterion(outputs.squeeze(), labels)  # Ajustement pour correspondre à la taille du batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        acc = accuracy(train_loader, model)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_acc.append(acc)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs, _ = model(inputs.unsqueeze(1), model._init_hidden(inputs.size(0)))\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            _, predicted = torch.max(outputs.squeeze(0).data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc1 = correct / total\n",
    "    test_loss.append(loss)\n",
    "    test_acc.append(acc1)\n",
    "\n",
    "    # record best train and test\n",
    "    if acc > best_train_acc:\n",
    "        best_train_acc = acc\n",
    "    if acc1 > best_test_acc:\n",
    "        best_test_acc = acc1\n",
    "\n",
    "print(f'valid_loss = {100 * loss / total : .3}% \\nvalid_acc = {100 * best_test_acc : .3}%')\n",
    "print(\"Entraînement terminé.\")\n",
    "# Sauvegarder le modèle entraîné\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage du courbe d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8gAAAFfCAYAAACSruXQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABy6ElEQVR4nO3deXhU9fn38ffMJDPZFyAkLIGAIousshWpipWKqNR9QVSglT5aUBG1yE/FrYKtG7hUKy3uFqvFpUJVGkFlkR0U2RUISxIIkD2Zycyc54+TTBJIIIEkk5n5vK5rrpk5c5b7nEBO7rm/i8UwDAMRERERERGREGf1dwAiIiIiIiIizYESZBERERERERGUIIuIiIiIiIgASpBFREREREREACXIIiIiIiIiIoASZBERERERERFACbKIiIiIiIgIAGFNfUCv18uBAweIjY3FYrE09eFFRESqMQyDgoIC2rZti9Wq740bgu71IiLS3NT1ft/kCfKBAwdITU1t6sOKiIic0N69e2nfvr2/wwgKuteLiEhzdbL7fZMnyLGxsYAZWFxcXFMfXkREpJr8/HxSU1N99yc5fbrXi4hIc1PX+32TJ8gVTa3i4uJ00xQRkWZDTYEbju71IiLSXJ3sfq/OViIiIiIiIiIoQRYREREREREBlCCLiIiIiIiIAH7ogywiEsg8Hg9lZWX+DkPqyW63awonEREROSklyCIidWAYBllZWeTm5vo7FDkFVquVTp06Ybfb/R2KiIiINGNKkEVE6qAiOW7dujVRUVEa8TiAeL1eDhw4QGZmJh06dNDPTkRERGqlBFlE5CQ8Ho8vOW7ZsqW/w5FTkJSUxIEDB3C73YSHh/s7HBEREWmm1CFLROQkKvocR0VF+TkSOVUVTas9Ho+fIxEREZHmTAmyiEgdqWlu4Arln90333zDqFGjaNu2LRaLhY8//vik2yxZsoRzzjkHh8PBmWeeyRtvvNHocYqIiDQHSpBFRESCWFFREX369OHll1+u0/q7du3isssu48ILL2TDhg1MnjyZ2267jS+++KKRIxUREfG/gO2D/OOBPB77z2aS4yJ4cXQ/f4cjIiLSLI0cOZKRI0fWef1XX32VTp068eyzzwLQvXt3li5dyvPPP8+IESNq3MbpdOJ0On3v8/PzTy9oCVger8Huw0VsyyrgUIGTgtIy8kvdVZ7dFDvd/g6zTqwWC7ERYcRGhBHlCKPU5SG/tIyCUjcer+Hv8ERCxm9/2YlLe7VpsuMFbIJcWuZh1a4jdGypPoEiIk0hLS2NyZMnM3nyZL/uQxrXihUrGD58eLVlI0aMOOHPbObMmTz22GONHJn4y94jxWzLKuBAXgn7c0s4lO/0Jb1Frspk1+0xk+PSMq8foxWRYHN576ZLjiGAE2RHmA0wE2URETnesGHD6Nu3L7NmzWqQ/a1evZro6OgG2Zc0X1lZWSQnJ1dblpycTH5+PiUlJURGRh63zbRp05gyZYrvfX5+PqmpqY0eqzSMzLwS3luZwTfbD5EcF0G3NnGckRTN5gP5/G9LNj8dKqrX/iLCrXRNjqVtQiRxEeHlVVjzOS4ynGi7jUAYFsDtNSgsdZNfWkaR00OU3eY7j3BbAJyASJDo3iauSY8XsAlyRLjZfdrp1reUIiKnyjAMPB4PYWEnvx0kJSU1QUQSiBwOBw6Hw99hSB2VlnnYebCQLZn5pG85yKIt2VWaDOfx5ebsauuHWS10TYmlfWIkbRMiSY6LIC4inLjIMKLtYb5k12Kx0KFFFB1aRGGzKoEUkcAUsAmyKsgi4i+GYVDip989keG2Oo3IPG7cOL7++mu+/vprZs+eDZiDL+3evZsLL7yQhQsX8tBDD/HDDz/w5ZdfkpqaypQpU/juu+8oKiqie/fuzJw5s1pT22ObR1ssFubMmcOCBQv44osvaNeuHc8++yy/+c1v6nw+GRkZ3HnnnaSnp2O1Wrnkkkt48cUXfRXMjRs3MnnyZNasWYPFYqFLly787W9/Y8CAAezZs4dJkyaxdOlSXC4XaWlpPP3001x66aX1uKJyrJSUFLKzqydI2dnZxMXF1Vg9luYtr7iMtRlH+HF/PluzC9iamc+unCKO7UI7uFMLru3fnvxSN9uy8tl5sJAOLaK4qHsy55+VRHyk5g8XkdAQuAlylQqyYRghPYWHiDStkjIPPab7Z0TfzY+PIMp+8l/ds2fPZvv27fTs2ZPHH38cMCvAu3fvBuCBBx7gmWeeoXPnziQmJrJ3714uvfRSnnzySRwOB2+99RajRo1i27ZtdOjQodbjPPbYY/zlL3/h6aef5sUXX2TMmDHs2bOHFi1anDRGr9fLFVdcQUxMDF9//TVut5uJEydyww03sGTJEgDGjBlDv379eOWVV7DZbGzYsIHwcPMP9YkTJ+Jyufjmm2+Ijo5m8+bNxMTEnPS4cmJDhgxh4cKF1ZYtWrSIIUOG+Ckiqa+fDxXy5vLdrNx1hK1ZBTWukxgVTreUOHq1j+fa/u05Kzm2iaMUEWme6pUgezweHn30Ud555x2ysrJo27Yt48aN46GHHmryBDUi3KwgGwa4PF5fRVlERCA+Ph673U5UVBQpKSnHff7444/z61//2ve+RYsW9OnTx/f+iSee4KOPPuLTTz9l0qRJtR5n3LhxjB49GoAZM2bwwgsvsGrVKi655JKTxpiens4PP/zArl27fP1V33rrLc4++2xWr17NwIEDycjI4P7776dbt24AdOnSxbd9RkYG11xzDb169QKgc+fOJz1mKCosLGTnzp2+97t27WLDhg20aNGCDh06MG3aNPbv389bb70FwO23385LL73EH//4R37729/y1Vdf8a9//YsFCxb46xTkBDxew9ecOTu/lFn/28G/1uytNspy51bR9G4fT/c2cXRrE0e3lFhaxzpUXBARqUG9EuQ///nPvPLKK7z55pucffbZrFmzhvHjxxMfH89dd93VWDHWyBFWOYVzaZkSZBFpOpHhNjY/XvN0N01x7IYwYMCAau8LCwt59NFHWbBgAZmZmbjdbkpKSsjIyDjhfnr37u17HR0dTVxcHAcPHqxTDFu2bCE1NbXaYE49evQgISGBLVu2MHDgQKZMmcJtt93G22+/zfDhw7nuuus444wzALjrrru44447+PLLLxk+fDjXXHNNtXjEtGbNGi688ELf+4rBtMaOHcsbb7xBZmZmtZ9zp06dWLBgAffccw+zZ8+mffv2/P3vf691iifxj+z8Uu7853pW7TpCZLiNuMgwcovLfGOz/Kpba67r354BaS1IilX/cBGRuqpXgrx8+XKuuOIKLrvsMsDsj/bPf/6TVatW1bpNY82NaLdZsVjMCrLT7QHUN0ZEmobFYqlTM+fm7NjRqO+77z4WLVrEM888w5lnnklkZCTXXnstLpfrhPupaO5cwWKx4PU23OCJjz76KDfddBMLFizgv//9L4888gjz5s3jqquu4rbbbmPEiBEsWLCAL7/8kpkzZ/Lss89y5513Ntjxg8GwYcMwjNrnbH3jjTdq3Gb9+vWNGJWcjh/25XHbW6vJzjf/viop8/jGRejfMZGpl3RjUKeTd3MQEZHjWU++SqVzzz2X9PR0tm/fDpiDpyxdupSRI0fWus3MmTOJj4/3PRpq2geLxUJEedXYqfn2RESOY7fb8XjqNpjYsmXLGDduHFdddRW9evUiJSXF11+5sXTv3p29e/eyd+9e37LNmzeTm5tLjx49fMvOOuss7rnnHr788kuuvvpqXn/9dd9nqamp3H777cyfP597772XOXPmNGrMIv624PtMrvvbcrLznZzZOob/3n0e39x/IZ/d+UsW3nUeH94+RMmxiMhpqFcJ5IEHHiA/P59u3bphs9nweDw8+eSTjBkzptZtGnNuREe4lZIyT3kFWUREqkpLS2PlypXs3r2bmJiYEw6c1aVLF+bPn8+oUaOwWCw8/PDDDVoJrsnw4cPp1asXY8aMYdasWbjdbv7whz9wwQUXMGDAAEpKSrj//vu59tpr6dSpE/v27WP16tVcc801AEyePJmRI0dy1llncfToURYvXkz37t0bNWYRfylxeZixcAtvf7cHgGFdk3hhdD/iItSCTkSkIdUrQf7Xv/7Fu+++y3vvvcfZZ5/Nhg0bmDx5Mm3btmXs2LE1btOYcyOaFeQySlVBFhE5zn333cfYsWPp0aMHJSUl7Nq1q9Z1n3vuOX77299y7rnn0qpVK6ZOndpgXWJqY7FY+OSTT7jzzjs5//zzq03zBGCz2Th8+DC33nor2dnZtGrViquvvprHHnsMMAeOnDhxIvv27SMuLo5LLrmE559/vlFjFvGHzQfyuWveenYeLATg9+d35o8juhJmq1dDQBERqQOLcaKOScdITU3lgQceYOLEib5lf/rTn3jnnXfYunVrnfaRn59PfHw8eXl5xMXF1T/iKi54ejF7Dhfz4e1DGJCm5kQi0jhKS0vZtWsXnTp1IiIiwt/hyCk40c+wIe9LYtI1PT17Dhfxn40H2JJZwJasfHaXz1ucFOvg2ev6cP5ZSf4OUUQk4NT13lSvCnJxcTFWa/VvK202W6M3w6uNrw+yWxVkERERCWxbs/J5ZclP/GfjAbzHlC9+3SOZp67uRcsYjUgtItKY6pUgjxo1iieffJIOHTpw9tlns379el+zPH+ICDeT9dIy9UEWERGRwJRfWsYjn/zIR+v3+5ad16UV53VpRdeUOLqnxNI6Tq1XRESaQr0S5BdffJGHH36YP/zhDxw8eJC2bdvy//7f/2P69OmNFd8JVcx9rD7IIiIiEohW7z7C5Hkb2J9bgsUCl/Zqwx0XnEHPdvH+Dk1EJCTVK0GOjY1l1qxZzJo1q5HCqR9HeQVZo1iLiIhIoHl58U6e/XIbXgM6tIhi1o19OadDor/DEhEJafVKkJubiHBVkEVERCTwLP8ph6e/2AbA1ee047HfnE2spmwSEfG7gE6QHWGqIIuIiEhg8XoNnlywBYCbBndgxlW9/ByRiIhUCOgJ9FRBFhERkUAzf/1+fjyQT6wjjHt/fZa/wxERkSoCOkGuqCBrFGsREREJBCUuD8+UN62e+KszNW2TiEgzE9AJckUFWfMgi4g0L8OGDWPy5Mn+DkOk2Znz7c9k5ZfSLiGSceem+TscERE5RoAnyKogi4jUpjGS1HHjxnHllVc26D5FQkGR08073+3h1a9/AuCBkd18X/SLiEjzEeCDdKmCLCIiIs1XkdPN019s48O1+yh0ugEYlNaCy3u38XNkIiJSk6CoIDtVQRYRqWbcuHF8/fXXzJ49G4vFgsViYffu3QBs2rSJkSNHEhMTQ3JyMrfccgs5OTm+bT/88EN69epFZGQkLVu2ZPjw4RQVFfHoo4/y5ptv8sknn/j2uWTJkjrFc/ToUW699VYSExOJiopi5MiR7Nixw/f5nj17GDVqFImJiURHR3P22WezcOFC37ZjxowhKSmJyMhIunTpwuuvv95g10qkMT25cAtvLN9NodNNp1bRPHx5D14fPxCLxeLv0EREpAZBUUEu1TRPItKUDAPKiv1z7PAoqMMf1rNnz2b79u307NmTxx9/HICkpCRyc3P51a9+xW233cbzzz9PSUkJU6dO5frrr+err74iMzOT0aNH85e//IWrrrqKgoICvv32WwzD4L777mPLli3k5+f7EtQWLVrUKexx48axY8cOPv30U+Li4pg6dSqXXnopmzdvJjw8nIkTJ+Jyufjmm2+Ijo5m8+bNxMTEAPDwww+zefNm/vvf/9KqVSt27txJSUnJKV5AkaaTU+jkw7X7AJh9Y19G9W6L1arEWESkOQvoBLmygqwm1iLShMqKYUZb/xz7/w6APfqkq8XHx2O324mKiiIlJcW3/KWXXqJfv37MmDHDt2zu3Lmkpqayfft2CgsLcbvdXH311XTs2BGAXr0q52iNjIzE6XRW2+fJVCTGy5Yt49xzzwXg3XffJTU1lY8//pjrrruOjIwMrrnmGt+xOnfu7Ns+IyODfv36MWDAAADS0tLqfGwRf3pr+W5cbi99UxP4TZ+2qhqLiASAAG9irQqyiEh9bNy4kcWLFxMTE+N7dOvWDYCffvqJPn36cNFFF9GrVy+uu+465syZw9GjR0/rmFu2bCEsLIzBgwf7lrVs2ZKuXbuyZcsWAO666y7+9Kc/MXToUB555BG+//5737p33HEH8+bNo2/fvvzxj39k+fLlpxWPSFMocXl467s9APz+/M5KjkVEAkRAV5Ar5kFWBVlEmlR4lFnJ9dexT0NhYSGjRo3iz3/+83GftWnTBpvNxqJFi1i+fDlffvklL774Ig8++CArV66kU6dOp3XsE7ntttsYMWIECxYs4Msvv2TmzJk8++yz3HnnnYwcOZI9e/awcOFCFi1axEUXXcTEiRN55plnGi0ekdP14dq95BaXkdoikhFn173FhYiI+FdAV5AdqiCLiD9YLGYzZ3886lGFstvteDzVfz+ec845/Pjjj6SlpXHmmWdWe0RHR5efnoWhQ4fy2GOPsX79eux2Ox999FGt+zyZ7t2743a7WblypW/Z4cOH2bZtGz169PAtS01N5fbbb2f+/Pnce++9zJkzx/dZUlISY8eO5Z133mHWrFm89tpr9YpBpCl5vAZ/X7oLgNt+2Rmb+h2LiASMwE6QwyrmQVYFWUTkWGlpaaxcuZLdu3eTk5OD1+tl4sSJHDlyhNGjR7N69Wp++uknvvjiC8aPH4/H42HlypXMmDGDNWvWkJGRwfz58zl06BDdu3f37fP7779n27Zt5OTkUFZWdtI4unTpwhVXXMGECRNYunQpGzdu5Oabb6Zdu3ZcccUVAEyePJkvvviCXbt2sW7dOhYvXuw75vTp0/nkk0/YuXMnP/74I5999pnvM5Hm6Msfs9hzuJj4yHCuG9De3+GIiEg9BHSCXNEH2akKsojIce677z5sNhs9evQgKSmJjIwM2rZty7Jly/B4PFx88cX06tWLyZMnk5CQgNVqJS4ujm+++YZLL72Us846i4ceeohnn32WkSNHAjBhwgS6du3KgAEDSEpKYtmyZXWK5fXXX6d///5cfvnlDBkyBMMwWLhwIeHh4QB4PB4mTpxI9+7dueSSSzjrrLP461//CphV62nTptG7d2/OP/98bDYb8+bNa5yLJnKail1unvlyGwC3/KIjUfaA7s0mIhJyLIZhGE15wPz8fOLj48nLyyMuLu609rX5QD6XvvAtSbEOVj84vIEiFBGprrS0lF27dtGpUyciIiL8HY6cghP9DBvyviSmUL6mUz/8nvfX7CU5zsHnd59PYrTd3yGJiAh1vzcFdAXZEV7RxFoVZBEREfGv/2w8wPtr9mKxwPM39FVyLCISgAI6Qa5sYq0+yCIiIuI/e48U83/zfwBg4rAzOfeMVn6OSERETkVAJ8gVg3S53F683iZtKS4iIiICgNdrcPe89RQ43fTvmMjk4V38HZKIiJyigE6QKyrIAC6PqsgiIiLS9D7esJ91GbnEOMKYfWNfwmwB/eeViEhIC+jf4BFhleGrH7KIiIg0tRKXh6e/MEet/sOFZ9A+McrPEYmIyOkI6AQ5zGbFZrUAmgtZRBqf16vfM4GqiSdskBDyj6U/k5lXSruESH47tFPjH/DQdji6p/GP09QMA7I3Q0H28Z8VHoIDG5o8JBEJTQE/OV9EmJUil0dzIYtIo7Hb7VitVg4cOEBSUhJ2ux2LxeLvsKSODMPg0KFDWCwW37zLIg3hYEEpf13yEwB/vKRrta5fjWL9u/DpJPN1jyvhvCmQ0qtxj9nYvF7Y+hl8+yxkbgBrOPQdDUMngy0clr0A698Gdyn8cgpcNB30+1dEGlHgJ8jhNopcHlWQRaTRWK1WOnXqRGZmJgcOHPB3OHIKLBYL7du3x2Zr5ARGQsrzi7ZT7PLQJzWB3/Rp23A7zvjOTAy7XQa9rzcTxVVzYOF9lev8ON98tOkDYZHH78MaBl1HQv9x4IhpuNga0qFt8P4tkGM2UccaBt4yWPcWrH8HLFbwuivXX/oclBXDJU+Z77d/Dj98CC3PhEG/h+iW9Tt+xnew8lXIz6xcFt8efvEHaN//9M5NRAJWwCfIFSNZq4IsIo3JbrfToUMH3G43Ho9+3wSa8PBwJcfSoLZk5vP+6r0APHxZ94ZrVVKUYyaNRQdh2wJYMhM6nQ8b3jU/H3wH9L0Jls2CHz+CzI2172vPUvj2GXObzhcA5THGt4f4dg0Tb23y9oEjFiLia/68rBT+NdZMjh3xMPj3MPh2OLwTvn0OdnwBhhc6D4Pz7oWc7bDgXjOhzdsHR3bBwR8r97f8Beg/HrpfDpaT/F8vOgTf/RX2LDv+s73Apg+h0wVmPFHlSXd4pFmtbw7V66IcOPzTydezWMu/QDnBfNyGAVk/QFlJzZ+3PLP+XzyIBLiAT5ArmjOpgiwija2iia6a6YqENsMwePjjTXgNuKxXGwaktWioHcOnd5nJcUIHM4nM21uZHJ9/P1z4oJmkXTsXfvWwmdzUpCALVr4CR36GJTPMRwWLFc6+2myinXx2w8ReYf9aM8Hd+hnEpMCtn0Drbsevl/4YHNoC0a3hjmUQ09pcHt0KxvzLTAC9Hkg6y1ze6XwIj4JPJpr7BrDHms2xM76DrO/hu5fNR11VNOc+czhgMRPy7V/A9+/Drq/NR1VnXQLXvQnhEfW+LA3iyC7zi4D174DHVbdtWp1l/gziamjhUFYKH4yD7f+tfXubA865Bc69CxI7nlLYIoEm4BNke3kFWaNYi4iISFOYv24/a/YcJTLcxoOXdW+4Ha9/26waW8PhxvegZRfY8A5s+Cf0uhZ+cUf19Vt0Mh+1Gfg72PwxrHwNCssHvzI8kJthVkk3fQgdh0JEwvHbWixwzq1w1oi6xV54ED66HX5Kr7IsC14fCbd8BG37Vi7/abFZwQW44qXK5Liqlmccv6zvTWZVeunzZrI6aAJEJppfLPyUDstfgqO7Tx6r1QZdLoYhk46vpJ99JQx7AJa/CD8vNpN0gPz9ZpPu966DG//Z+M3Wj+6Gb56G4qPm+7Ji2PWN+fMDiGtvNr0/keLDZuV97iUw9lNITKv8zFkI824yvwSw2SGuhhYFHpd53qv/DmteN6//yL+AvYlHave4ze4EWz8zX/uLxQJp55lfGNij/ReHNDqL0cRDe+bn5xMfH09eXh5xcXGnvb8rX17Ghr25zLl1AL/ukdwAEYqISChp6PtSc/Tyyy/z9NNPk5WVRZ8+fXjxxRcZNGhQjeuWlZUxc+ZM3nzzTfbv30/Xrl3585//zCWXXFLn4wXzNc0rKeOiZ5eQU+hi6iXduGNYDYncqTjyM7zySygrguGPwS8nN8x+a5K50Uwyf/wYOMGfgREJcPdGiEw48f4MA965xkxSLTaz33T/8fD5VDiwHhxxZsU7sZM52Na710HBARjwW7j8+YY7r8a0eym8dwO4CqH9IBjzwcmvy6k6tB3eusK8Rsc64yKzyXnHc0/e3PvoHnM/R3dBbFu47nWIamX28/7P3bB3JdhjYPQ86HTe8dsbhnne3z5rflkA5hcqo+dBRCP9v3YWVBnJ3IDd38LSWZDbjEZuj2ppflnV/Tcnb87fmCLiISbJf8cPQHW9NwV8gnzjayv47ucjvDi6H6MacoAMEREJCcGczAG8//773Hrrrbz66qsMHjyYWbNm8cEHH7Bt2zZatz6+cjd16lTeeecd5syZQ7du3fjiiy+YMmUKy5cvp1+/fnU6ZjBf00c//ZE3lu+mc1I0n999vq8l22kpyoG3roTsH8wEZOx/zCpnY8vZCRnLzabFx1r+EhzeYTbr/tVDJ97Pytfgv/dDWATc9r/KkbVL8+G96yFjxfHbtDgDbv82sCpx+9bAO1dDaR50GALjFjT8zynze3j7KijOgaTuMPj/VSbCbfuZfYrrIz8T3r4SDm09/rOIeLh5PrQfcPL97PoG5o0BZz606w9jPoSoBupaAJC3H1a8BGvfMKvlx4pqZbaIqKmpeFMpzYM1c+vWSqFJWKDHb8zR3au20JBahUyCPHbuKr7efohnruvDtf3bN0CEIiISSoI5mQMYPHgwAwcO5KWXXgLM+bxTU1O58847eeCBB45bv23btjz44INMnDjRt+yaa64hMjKSd955p8ZjOJ1OnE6n731+fj6pqalBd01/PJDHqBeX4jXg7d8N4rwuDVC9yT9gJsc52yA6CSZ8ZfY/9rct/4H3b4bwaLh7Q83NoMEcifpv55uV4ZF/MRO6qlxFZr/qnYsqlzni4Ya3zIQv0GT9AHNHgqvA7AN+/n0n3+bQNnNk7tK8k6xomNe9NM9MhG/+qGEGyCo6DPMnwP41lcviU+GqV+s3TdiB9fD21VByxOzbnDr49GMDM+neutCsbIPZt9xa/sVTdJI5Qnm/W5q+aXdNPG6z28LyF83KvD9V/ffU6YL6/96wR0OfGwPz/+Epquv9PuD7IEeEqw+yiIhITVwuF2vXrmXatGm+ZVarleHDh7NiRQ1VPcxkNyKi+iBEkZGRLF26tNbjzJw5k8cee6xhgm6mSss83PuvjXgNuLRXSsMkx0f3wFu/MStSce3MwZSaQ3IM0O1yaHsOHFhnDro1snxqJcOA0lzz2fCaiZe71Gz6O3DC8fuxR8O1/2jS0BtVSi+49Gn4+HZzhPEzfgXtzjE/q3ptwBxUa9nzsOUzTtiU/Vipg80m3LWNAF5f0S3hlvmnv5+2/cyq+dtXmn2bc7af/j6rSjvPHDiu84XNY7TwmtjCzPEAel3r70gge7PZVWLTh8cPKFdXK181/w3/8jQH7LNYzC4ZJ/u5OQvAU1b//YdHNengeAGfIDvCKkaxVoIsIiJSVU5ODh6Ph+Tk6mN0JCcns3VrDU0ugREjRvDcc89x/vnnc8YZZ5Cens78+fNPOL3ZtGnTmDJliu99RQU5mDzzxTa2ZhXQKsbO41f0PP0d5uww+4fm7zcHT7r10+Y1SrDFAhdNN5OhNf8wp2Has9zsD3p4R/V1IxPhipcrq37Brs+N5sjPmz+B+b83q/7bPze/SDi0peZtul1emUifSGQi9L6h+TY9T+4BExabU4x5nCdfv04skPZLSK15XASpRXIPuGYOXPh/5QOY1XFk8woHt8Kmf8NPX5mP05XUDX55D/S8pvoAcjX1Za+vmlqnNKKAT5ArKshOt6Z5EhEROV2zZ89mwoQJdOvWDYvFwhlnnMH48eOZO3durds4HA4cDkcTRtm0lu/M4e9LzeaUf76mN61iTvNcszaZiWfRIWjVtXwanjanH2hD6zzMrOrt/hZeGlTZBLaqsAi44q/NM/7GYrHA5bMgY6X5ZcGz3czB1Y5lDTeThV/eU/NUV4Eqvh2cO8nfUUiFFp3g3DtPbdtfPWg2F1//LrhrmQu7rg5thY/+Hyx+0hzAzFqeZu5ZDvtWnd6+m1gQJMhmBdmpCrKIiEg1rVq1wmazkZ2dXW15dnY2KSkpNW6TlJTExx9/TGlpKYcPH6Zt27Y88MADdO7cuSlCbnbyisu494ONAIwe1IGLup/mjBn715r9OEtzIaW3OQVSdKvTD7QxWCxw0SPwj+FmchyTbE6N1H+sOfqxuVLoVI6rimoBV/7VHLSrrKh8ZOM/mANJOSr6NobotZHAkZgGlz0LI5+mXt0AjuUsMAcwW/GyOY3cipeqf+6bT/tOs/97vTVtk/uAT5AdFfMgq4IsIiJSjd1up3///qSnp3PllVcC5iBd6enpTJp04gpQREQE7dq1o6ysjH//+99cf/31TRBx8/PnL7aSmVdKWssoHjrdOY93LyufKqig8acKaiipA+H6t8x5c3te06T9AJu9My+CG96B4iPQ67rmMYiUyKk43S9yIhPM/uODb4fv34fDOys/i2oBfW+G2MCZjjfgE2RVkEVERGo3ZcoUxo4dy4ABAxg0aBCzZs2iqKiI8ePHA3DrrbfSrl07Zs6cCcDKlSvZv38/ffv2Zf/+/Tz66KN4vV7++Mc/+vM0/CIrr5QP1uwF4KlrehPtqOOfTXn7zT53XS+pHGhp5/9g3s1mM8a088y5ZB0xJ95Pc9HjCn9H0Hx1H+XvCESaD3sUDBjv7yhOW8AnyL4KcpkqyCIiIse64YYbOHToENOnTycrK4u+ffvy+eef+wbuysjIwFqlelBaWspDDz3Ezz//TExMDJdeeilvv/02CQkJfjoD/5m7bBdlHoOBaYn8onMdp9vxeuDda+HgZrOp7aAJ0LIL/OcucxCdLhebFdnwyMYNXkRETknAJ8i+CrJbFWQREZGaTJo0qdYm1UuWLKn2/oILLmDz5s1NEFXzlldcxrvf7QHgjmFn1H3DHz4wk2Mw53f99tnKz3pcAVf/HcLsDRipiIg0pIAfOcARXjHNkyrIIiIi0jDeWbmHIpeHrsmxXNi1dd02crtg8Qzz9UXT4YZ3zbmEAfrcBNfMVXIsItLMBXwFuXKQLlWQRURE5PSVlnl4fZk5rdPtwzpjsdRxBNX1b0HuHnO058G3m3PZdrsMCg8G1AA1IiKhLOAryJWDdKmCLCIiIqfvg7X7yCl00S4hkst7t63bRq5i+Ppp8/X595vJMZhTJSk5FhEJGAFfQY5QBVlEREQaSJnHy2vf/ATAbed1Itx2glpC1g+Qt898/dNiKMyChA5wztgmiFRERBpDwCfIDlWQRUREpIH8e+0+9h4poVWMnRsGpta+4t7VMPdiMI75+2PY/6mfsYhIAAv4BFkVZBEREWkILreXF7/aCcDtF5xBlL2WP5MMA9IfM5PjhA4QXT6IV0pP6H19E0UrIiKNIeATZFWQRUREpCH8a81e9ueW0DrWwc2/6Fj7ij8vgd3fgs0O4xaYSbKIiASFIBikyzwFzYMsIiIip6q0zMPLi83q8cQLz/QNAnocw4D0x83XA36r5FhEJMgEfoIcpnmQRURE5PS8v3ovmXmltImPOHHf462fwYF1EB4N593bdAGKiEiTCPgE2VFeQS4tUwVZRERE6i87v5SX6lI99nrgqz+Zr39xB8S0bqIIRUSkqQR8glxRQXZ7DdweVZFFRESk7vYeKea6V1dwqMBJWssorh9wgurxujfh0FaISIBz72yyGEVEpOkEwSBdlTm+0+0l7ETzFYqIiIiU++lQIWPmrCQrv5SOLaN4+3eDsYfV8nfE4Z/gi4fM1xdMhciEJotTRESaTsAnyBUVZDAT5GiHH4MRERGRgJCZV8INf1tBTqGLLq1jeOe2wSTHRdS8sscN838PZUWQdh4Mvr1pgxURkSYT8OVWq9WC3aZ+yCIiIlJ3H6zZR06hi7OSY5j3+1/UnhwDfPss7F8Djni48hWwBvyfTyIiUoug+A3vCFOCLCIiInX31daDAPx2aCdaxpyg+dm+tfD1n83Xlz0DCSfooywiIgEvOBLk8tEmnW4N0iUiIiInllPoZOO+XAAu7HaCkagNAxbeC4YHzr4ael3XNAGKiIjfBEeCrAqyiIiI1NGSbYcwDOjZLu7ETau3/AcOrAd7DIz8C1gsTRekiIj4RVAkyBHlI1mrgiwiIiIns7i8efWvup6gelxtzuM/QExSE0QmIiL+FiQJstnEWhVkEREROZEyj5dvth8CTtK8+vv3IWcbRCbCuZOaKDoREfG3eifI+/fv5+abb6Zly5ZERkbSq1cv1qxZ0xix1VllE2tVkEVERKR2q3cfocDppmW0nT7tE2peye2ExTPN17+8ByLimyw+ERHxr3rNg3z06FGGDh3KhRdeyH//+1+SkpLYsWMHiYmJjRVfnUT4BulSBVlERERq99UWs3n1sK6tsVpr6VO87i3Iy4CYFBg4oQmjExERf6tXgvznP/+Z1NRUXn/9dd+yTp06nXAbp9OJ0+n0vc/Pz69niCdXUUF2qoIsIiIiJ/DVtvL+x7U1rzYMWDbbfH3B/WCPaqLIRESkOahXE+tPP/2UAQMGcN1119G6dWv69evHnDlzTrjNzJkziY+P9z1SUxt+/kBVkEVERORkMjem80Le3fzPcR+XfD0KXv4FbHy/+kpZ30PeXgiPgr43+ydQERHxm3olyD///DOvvPIKXbp04YsvvuCOO+7grrvu4s0336x1m2nTppGXl+d77N2797SDPlblIF2qIIuIiEjNSpf9lZ7W3ZxpOYDt8A44tAX+9yh4q/z9sO1z8/mMX0H4CaaAEhGRoFSvJtZer5cBAwYwY8YMAPr168emTZt49dVXGTt2bI3bOBwOHA7H6Ud6ApoHWURERE4m9vD3ACw9ayq/PPd8mHcTFByAjOWQ9ktzpe3/NZ/PGuGnKEVExJ/qVUFu06YNPXr0qLase/fuZGRkNGhQ9VXZxFoVZBERETmeMzeTVp6DeA0LSUPHmglx99+YH/7wgfmcnwkH1puvuyhBFhEJRfVKkIcOHcq2bduqLdu+fTsdO3Zs0KDqyxGuCrKIiIjU7ueN3wKwy9KOszq0NRf2utZ83vwJuF2w40vzfbv+EJvshyhFRMTf6pUg33PPPXz33XfMmDGDnTt38t577/Haa68xceLExoqvThxh5X2QNUiXiIiI1ODI9hUA5MT3xGIpn94p7TyISYaSo/DTV7C9vP/xWSP9FKWIiPhbvRLkgQMH8tFHH/HPf/6Tnj178sQTTzBr1izGjBnTWPHVSUS4pnkSERGR2kUc3ACAvcOAyoVWG5x9tfl6/dvw02Lztfofi4iErHoN0gVw+eWXc/nllzdGLKessoKsBFlERESqO1Lo5AzXNrBAx17nVf+w13Ww8hXY+pn5Pq49pPRq+iBFRKRZqFcFubmqrCCribWIiIhUt37jOhIsRbgIo0Xnc6p/2O4cSOxU+f6sEVDRBFtEREJOcCTIqiCLiIhILbK3mP2PD0Z3hTB79Q8tlsrBugC6qv+xiEgoC4oEWaNYi4iISE0Mw8CWuc580/acmlfqdR1YrOCINwfuEhGRkFXvPsjNUUUFWfMgi4iISFU/5xRxRtk2sELrbufWvFJSV7jlY4iIg/CIJo1PRESal6BIkB3qgywiIiI1WLYtk+stuwCwdxxY+4qdL2iiiEREpDkLiibWEeGqIIuIiNTm5ZdfJi0tjYiICAYPHsyqVatOuP6sWbPo2rUrkZGRpKamcs8991BaWtpE0TasXZvXEmEpw2mLgRZn+DscERFp5oIjQa4YpEsVZBERkWref/99pkyZwiOPPMK6devo06cPI0aM4ODBgzWu/9577/HAAw/wyCOPsGXLFv7xj3/w/vvv83//939NHHnDCM82+x87k/uCNSj+7BERkUYUFHcKDdIlIiJSs+eee44JEyYwfvx4evTowauvvkpUVBRz586tcf3ly5czdOhQbrrpJtLS0rj44osZPXr0SavOzVFOoZPOzm0ARHQc4OdoREQkEARFgqxBukRERI7ncrlYu3Ytw4cP9y2zWq0MHz6cFStW1LjNueeey9q1a30J8c8//8zChQu59NJLaz2O0+kkPz+/2qM52JpZQC9ref/jDkqQRUTk5IJqkK7SMg+GYWCxWPwckYiIiP/l5OTg8XhITk6utjw5OZmtW7fWuM1NN91ETk4Ov/zlLzEMA7fbze23337CJtYzZ87ksccea9DYG8LWzDxGW7LMN0nd/BuMiIgEhKCqIHsNcHsNP0cjIiISuJYsWcKMGTP461//yrp165g/fz4LFizgiSeeqHWbadOmkZeX53vs3bu3CSOu3b59e4i2OPFihYQO/g5HREQCQFBVkMGsIofbgiLvFxEROS2tWrXCZrORnZ1dbXl2djYpKSk1bvPwww9zyy23cNtttwHQq1cvioqK+P3vf8+DDz6ItYaBrhwOBw6Ho+FP4DQVZu4AwBnVhsiw5hefiIg0P0GRSTrCrFS0qi7RQF0iIiIA2O12+vfvT3p6um+Z1+slPT2dIUOG1LhNcXHxcUmwzWa21DKMwGmlVebxYjtq9j+2tOjs52hERCRQBEUF2WKxEG0Po9DppsjpgVh/RyQiItI8TJkyhbFjxzJgwAAGDRrErFmzKCoqYvz48QDceuuttGvXjpkzZwIwatQonnvuOfr168fgwYPZuXMnDz/8MKNGjfIlyoFgV04R7cgEwJF8pp+jERGRQBEUCTJAjMNMkAtL3f4ORUREpNm44YYbOHToENOnTycrK4u+ffvy+eef+wbuysjIqFYxfuihh7BYLDz00EPs37+fpKQkRo0axZNPPumvUzglWzLzSbOYTctVQRYRkboK3ATZ44bSPPC4IK4N0Q7zW+1CpxJkERGRqiZNmsSkSZNq/GzJkiXV3oeFhfHII4/wyCOPNEFkjWdLZgEjyxNklCCLiEgdBW4f5D3L4OnO8PZVAMREhANQpARZREQk5G3NyqejL0Hu5N9gREQkYARuguyIMZ+dBQDEqIIsIiIi5Q4cOECCpch8k5jm11hERCRwBHCCHGc+u8wEOdputhZXgiwiIhLajha5iCzcA4A3JgXs0X6OSEREAkXgJsj2igpyIRgGMRFKkEVERAS2VGlebW15hp+jERGRQBK4CXJFE2vDA+5SYhxmgqw+yCIiIqFta2aBbwRr9T8WEZH6CNwEObxKcylngS9BVgVZREQktG3NyqejNct8k6gEWURE6i5wE2SrFeyx5mtnAdEVCbLmQRYREQlpW7MK6Gg5aL7RFE8iIlIPgZsgQ2Uza1dhZRNrlxJkERGRUOXxGmzLKqCjpbyCrARZRETqIbAT5CoDdVU2sfb4MSARERHxp5xCJ+HuQpIs+eYC9UEWEZF6COwEucpcyJVNrMv8GJCIiIj4U3Z+aWXz6qhWEBHv34BERCSgBHiCXN4H2VVIbETFKNaqIIuIiISqg/lONa8WEZFTFtgJck2DdGkUaxERkZCVXVClgqzm1SIiUk+BnSBXG6TLBihBFhERCWXZqiCLiMhpCOwE2V7ZBznGEQ5AkdONYRh+DEpERET85WB+KWnWbPONEmQREamnwE6QHZWjWEeXV5DdXgOn2+vHoERERMRfDhY46WhRgiwiIqcmwBPkikG6Coi2h/kWq5m1iIhIaDqal0cbyxHzTaL6IIuISP0EdoLsG6SrEKvVQrS9vB9yqRJkERGRUBSWvxcAT3gsRLXwczQiIhJoAjtBrjIPMqCRrEVEREKY2+MltnQ/AEZCB7BY/ByRiIgEmsBOkO2Vo1gDxPjmQlaCLCIiEmpyCl2kYk7xZGup5tUiIlJ/gZ0gOyqbWAPEqIIsIiISsrLzS0ktnwPZkpjm32BERCQgBUmCnA8oQRYREQll2fmldChPkFGCLCIipyCwE+RjmlirD7KIiEjoOljgJNVyyHyjBFlERE5BYCfIVeZBhsoKsvogi4iIhJ6DeSW+JtYkdPRvMCIiEpACPEEub2LtcYLbVaWJtcePQYmIiIg/FB7NJsZSar5J6ODfYEREJCAFdoJcMQ8ygKuwsom15kEWEREJOUbuHgCKHa0hPMLP0YiISCAK7ATZFgZh5TdAZwGxmuZJREQkZDkKMgBwxal6LCIipyawE2SoNlBXtN0GaJAuERGRUBRTst98kZDm1zhERCRwBX6CXGUuZI1iLSIiEprKPF5aujIBsLdK828wIiISsIIgQa4YyVpNrEVEREJVTqHTN4J1ROsz/ByNiIgEqsBPkCsG6nIVqIIsIiISorLznXQoT5CtLdL8G4yIiASswE+Qq8yFHKMEWUREJCQdzC2kreWw+SYxza+xiIhI4Ar8BLnKIF1KkEVEREJT0cHdhFm8lFnCISbF3+GIiEiACvwE2TdIV2UTa/VBFhERCS3uw7sAyLW3AWvg/3kjIiL+Efh3kCoJckz5IF1lHgOn2+PHoERERKQpWXLNOZCLotr7ORIREQlkgZ8gV5sHOcy3uLBUVWQREZFQ4Sg0E2R3XAc/RyIiIoEs8BPkKoN02awWouw2AIqcqiCLiIgAvPzyy6SlpREREcHgwYNZtWpVresOGzYMi8Vy3OOyyy5rwojrL7bkAAAWjWAtIiKnIQgS5Mom1oCvH3KBs8xfEYmIiDQb77//PlOmTOGRRx5h3bp19OnThxEjRnDw4MEa158/fz6ZmZm+x6ZNm7DZbFx33XVNHHn9tCozE2RHq85+jkRERAJZ4CfIVZpYA76RrFVBFhERgeeee44JEyYwfvx4evTowauvvkpUVBRz586tcf0WLVqQkpLieyxatIioqKhmnSC73F7aGNkAxLY508/RiIhIIAv8BPmYCnKMRrIWEREBwOVysXbtWoYPH+5bZrVaGT58OCtWrKjTPv7xj39w4403Eh0dXes6TqeT/Pz8ao+mlHPkMC0t5t8BcW3OaNJji4hIcAn8BPmYCnK0w+yDXKAEWUREQlxOTg4ej4fk5ORqy5OTk8nKyjrp9qtWrWLTpk3cdtttJ1xv5syZxMfH+x6pqamnFXd95R3YYT4TgyUyoUmPLSIiwSXwE+TjKsjhgCrIIiIip+sf//gHvXr1YtCgQSdcb9q0aeTl5fkee/fubaIITcUHdwOQE5bSpMcVEZHgc1oJ8lNPPYXFYmHy5MkNFM4pqDKKNUBMeQVZ0zyJiEioa9WqFTabjezs7GrLs7OzSUk5cTJZVFTEvHnz+N3vfnfS4zgcDuLi4qo9mpL7qJmQF9iTT7KmiIjIiZ1ygrx69Wr+9re/0bt374aMp/7s5RXksiLwen2jWBeqgiwiIiHObrfTv39/0tPTfcu8Xi/p6ekMGTLkhNt+8MEHOJ1Obr755sYO87QZ+eYI1qVRqiCLiMjpOaUEubCwkDFjxjBnzhwSExNPuG6jD9xRUUEGcBUSE6FBukRERCpMmTKFOXPm8Oabb7JlyxbuuOMOioqKGD9+PAC33nor06ZNO267f/zjH1x55ZW0bNmyqUOut/CiTAC8sW38HImIiAS6U0qQJ06cyGWXXVZtVMzaNPrAHWERYDGbVeMsIMauCrKIiEiFG264gWeeeYbp06fTt29fNmzYwOeff+4buCsjI4PMzMxq22zbto2lS5fWqXl1cxBVYg44Zo1v7+dIREQk0IXVd4N58+axbt06Vq9eXaf1p02bxpQpU3zv8/PzGzZJtljMgbpKc8sryHZACbKIiEiFSZMmMWnSpBo/W7JkyXHLunbtimEYjRxVw4krOwiAo2XTjp4tIiLBp14J8t69e7n77rtZtGgRERERddrG4XDgcDhOKbg6q0iQnYVEO1oDSpBFRERCgmHQ0nsYgNjWHf0cjIiIBLp6NbFeu3YtBw8e5JxzziEsLIywsDC+/vprXnjhBcLCwvB4PI0V54n55kIuIMahPsgiIiKhoqzwMBG4AEhMVoIsIiKnp14V5Isuuogffvih2rLx48fTrVs3pk6dis1ma9Dg6sw31VNlglzo9FOyLiIiIk3maOYuWgM5Rhwt4mL9HY6IiAS4eiXIsbGx9OzZs9qy6OhoWrZsedzyJuUovyE6C4lOrEiQy/wXj4iIiDSJwkMZZoJsbUUrq8Xf4YiISIA75XmQmxVfE+tCYn3TPKmCLCIiEuxKDmcAkBfe2s+RiIhIMKj3KNbHqmn0yybnqyAXEF3RxLpUfZBFRESCnTd3PwDFEcl+jkRERIJBcFWQq8yD7PJ4cbm9fgxKREREGpu1wEyQy6JT/ByJiIgEg+BIkCsqyK5Coh2VA4VpJGsREZHg5ijOMl/EtfNvICIiEhSCJEGuqCAXEmazEhFunpbmQhYREQlu0c6DAIQltvdzJCIiEgyCI0GuMg8yQIwjHFCCLCIiEtQMg0T3IQAiW3XwczAiIhIMgiNBrjJIF0BMeTNrNbEWEREJYiVHicAJQHxrJcgiInL6gixBLgQgpnyqpwIlyCIiIkHLnbsPgMNGLEmJCf4NRkREgkJwJMhV5kEGiIswm1jnl5T5KyIRERFpZPkHzTmQM42WtIy2+zkaEREJBsGRIFcZpAsgMcq8SR4tcvkrIhEREWlkRYf2AHDU1gqr1eLnaEREJBgER4Jsr94HOSHKrCAfKVYFWUREJFiVHTGbWBc4Wvs5EhERCRbBkSA7qoxibRi0KG9mlVusCrKIiEiwMvL3A1AS2cbPkYiISLAIkgS5vIJseKGshISKJtaqIIuIiAStsMJMADwxSpBFRKRhBEeCHB4NlvJTcebTItpsYq0+yCIiIsErsjQLAFt8ez9HIiIiwSI4EmSrFSISzNfFR6pUkJUgi4iIBCXDIM51EAB7y1Q/ByMiIsEiOBJkgKiW5nPxYVpoFGsREZHgVppHhFEKQGySEmQREWkYQZkgJ6oPsoiISHArH6DriBFDq8QE/8YiIiJBIzgT5PI+yCVlHkrLPH4MSkRERBqDJ9ec4inTaEnrOIefoxERkWARRAlyC/O55AgxjjDCrBZA/ZBFRESCUdGhDACyjBa0jFaCLCIiDSP4EuTiI1gsFt9AXUfUD1lERCTolBzeC0BeeBK28i/FRURETlcQJciVTawB31RPueqHLCIiEnTK8sw5kEscSX6OREREgknQJsiqIIuIiASxQnOKJ3eUEmQREWk4QZggHwHwTfWUqz7IIiIiQSes2EyQiU3xbyAiIhJUgidBjqzog2xWkCtGstZUTyIiIsEn0pkDgDU22c+RiIhIMAmeBPmYCnKimliLiIgEJ6+X6DLzfm+LbePnYEREJJgEUYJcXkF2FYDb5UuQ1cRaREQkyJQcJQw3APYEVZBFRKThBE+CHJEAlvLTKTlCYnR5BVlNrEVERIJLYTYAR4wYEmKi/RyMiIgEk+BJkK3Wav2QE6MqpnlSBVlERCSoFGYBcNBI9I05IiIi0hCCJ0GGymbWxYc1zZOIiEiwKp/i6ZART3yk3c/BiIhIMAmyBLlyLuQW0RV9kNXEWkREJJh48jMBOEiCr8WYiIhIQwjSBPmI74ZZ6HTjcnv9GJSIiIg0JFeumSAfMhKIj1SCLCIiDSfIEuSKJtZHiIsIx2ox36ofsoiIhLKXX36ZtLQ0IiIiGDx4MKtWrTrh+rm5uUycOJE2bdrgcDg466yzWLhwYRNFe3LufLMPckFYC8JswfWnjIiI+Fdw3VWqDNJltVoq+yErQRYRkRD1/vvvM2XKFB555BHWrVtHnz59GDFiBAcPHqxxfZfLxa9//Wt2797Nhx9+yLZt25gzZw7t2rVr4shrZxSYo1gX21v5ORIREQk2Yf4OoEFV6YMMkBgVzpEiF0eL1A9ZRERC03PPPceECRMYP348AK+++ioLFixg7ty5PPDAA8etP3fuXI4cOcLy5csJDzebL6elpTVlyCdlLTKT+7IIJcgiItKwgquCfFyCXDFQlyrIIiISelwuF2vXrmX48OG+ZVarleHDh7NixYoat/n0008ZMmQIEydOJDk5mZ49ezJjxgw8Hk+tx3E6neTn51d7NCZ7ySEA3NHJjXocEREJPcGZIJccASAxWk2sRUQkdOXk5ODxeEhOrp5IJicnk5WVVeM2P//8Mx9++CEej4eFCxfy8MMP8+yzz/KnP/2p1uPMnDmT+Ph43yM1NbVBz6OashLs7gIALDGtG+84IiISkoIsQa7sgwz4RrLWVE8iIiJ14/V6ad26Na+99hr9+/fnhhtu4MEHH+TVV1+tdZtp06aRl5fne+zdu7fxAiw0+x87jXAcMS0a7zgiIhKSgrQPcnkFuWKQriJVkEVEJPS0atUKm81GdnZ2teXZ2dmkpKTUuE2bNm0IDw/HZrP5lnXv3p2srCxcLhd2u/24bRwOBw6Ho2GDr02h2f/4oJFAQvTxsYiIiJyO4KwguwrB7fQ1sT6qJtYiIhKC7HY7/fv3Jz093bfM6/WSnp7OkCFDatxm6NCh7Ny5E6/X61u2fft22rRpU2Ny3OTKK8iHiPd9ES4iItJQgitBdsSDpfwb7+IjvibWR1VBFhGREDVlyhTmzJnDm2++yZYtW7jjjjsoKiryjWp96623Mm3aNN/6d9xxB0eOHOHuu+9m+/btLFiwgBkzZjBx4kR/nUJ1BWbf6YNGIgnl93kREZGGElxNrK1Ws4pcdAiKD5MYZU7/cFR9kEVEJETdcMMNHDp0iOnTp5OVlUXfvn35/PPPfQN3ZWRkYLVWfl+emprKF198wT333EPv3r1p164dd999N1OnTvXXKVRX3sT6kBFPR1WQRUSkgQVXggwQWSVBjm4LqIm1iIiEtkmTJjFp0qQaP1uyZMlxy4YMGcJ3333XyFGdokKzgnzISKCPKsgiItLAgquJNVSbC7mib5KaWIuIiASJikG6SFAfZBERaXBBmCCXD9RVUtkHOb/UjdvjPcFGIiIiEgi8+RUV5Hj1QRYRkQYXhAly5VRP8ZHhWCzm29wS9UMWEREJdEb5KNaHLYnEOIKvp5iIiPhXECbI5RXk4sOE2azERZjfLueqH7KIiEhg83qxFh0CwBWRhKXiW3AREZEGEoQJcmUfZMDXzPpIkSrIIiIiAa3kCBbDDYC3fKYKERGRhhT8CXJ0+UBdqiCLiIgEtorm1UYssdFRfg5GRESCURAnyEcANJK1iIhIsCionOIpQSNYi4hIIwidBLlYTaxFREQCWsUUT0aCrwuViIhIQwq+BDky0Xwub2LdItq8gR4udPorIhEREWkIheUVZOJVQRYRkUYRfAlyRQW5rAjKSkmJjwQgM6/Uj0GJiIjIaSuvIJtNrFVBFhGRhhd8CXJEPFhs5uuSI7RLMBPk/bklfgxKRERETluVPsiJqiCLiEgjCL4E2WKpNpK1EmQREZEgUZwDwGEjTn2QRUSkUQRfggwQ1cJ8Lj5Mu0QzQT5U4MTp9vgxKBERETktriIACokkPlIVZBERaXjBmSBHJ5nPBVkkRoUTEW6eZmau+iGLiIgELFcxAMU4SIxWBVlERBpecCbICR3N59y9WCwWXzPrA2pmLSIiErAMVyEAJYZDfZBFRKRRBGmC3MF8zt0DQNvyBHmfEmQREZGAZZRXkIuIID5SFWQREWl4QZ4gZwDQPlEVZBERkYBXZvZB9oZFERFu83MwIiISjOqVIM+cOZOBAwcSGxtL69atufLKK9m2bVtjxXbqjkmQ25bPhbz/qBJkERGRgOT1YHWbY4nYI2P8HIyIiASreiXIX3/9NRMnTuS7775j0aJFlJWVcfHFF1NUVNRY8Z2axPI+yHn7wOvxjWR9IE8JsoiISEAqK/a9dETF+jEQEREJZmH1Wfnzzz+v9v6NN96gdevWrF27lvPPP7/GbZxOJ06n0/c+Pz//FMKsp9g2YA0DbxkUZPr6IKuCLCIiEqDKp3jyGhaiolVBFhGRxnFafZDz8vIAaNGiRa3rzJw5k/j4eN8jNTX1dA5ZN1YbxLc3X+dmVI5inVeK12s0/vFFRESkYZUnyMU4SIh2+DkYEREJVqecIHu9XiZPnszQoUPp2bNnretNmzaNvLw832Pv3r2nesj6qdIPOSU+AqsFXG4vOUXOE28nIiIizU95glyCgwSNYC0iIo2kXk2sq5o4cSKbNm1i6dKlJ1zP4XDgcPjhm94qCXK4zUpyXASZeaUcyC2ldWxE08cjIiIip668D3KREaE5kEVEpNGcUgV50qRJfPbZZyxevJj27ds3dEwNI6F8oK5j5kJWP2QREZEAVLWCHKUKsoiINI56JciGYTBp0iQ++ugjvvrqKzp16tRYcZ0+X4JsTvXk64esuZBFREQCT3mCXEQECaogi4hII6lXE+uJEyfy3nvv8cknnxAbG0tWVhYA8fHxREZGNkqAp+zYuZArKshKkEVERAJPeRPrYsNBbMQp9xATERE5oXpVkF955RXy8vIYNmwYbdq08T3ef//9xorv1FUkyMfMhawEWUREJAC5CgEoJoIYhxJkERFpHPW6wxhGAE2RFJsC1nBzLuT8A7RLMAfmUh9kERGRAOQqryDjIEUJsoiINJLTmge5WTtuLuQoAA7kKUEWEREJOBXzIBsOYhw2PwcjIiLBKngTZKjWD7lteQU5t7iMIqfbj0GJiIhIfRkVCTIRRKuCLCIijSS4E+TEypGsYyPCiSsf1EMjWYuIiAQWj9Psg1yCgyi7EmQREWkcwZ0g1zKS9T4lyCIiIgHFU2omyEWGg2i7mliLiEjjCPIEuaKCvAeA9omaC1lERCQQeZxmE+syWyRhtuD+80VERPwnuO8wvgqymSC3q5gLWSNZi4iIBBRveRNrb1iUnyMREZFgFhoJct5+8Lh9TaxVQRYREQksRvk0T95wJcgiItJ4gjtBjkkBmx0MDxQcoF15E+uMI8V+DkxERKTpvPzyy6SlpREREcHgwYNZtWpVreu+8cYbWCyWao+IiIgmjLYWLrOCbIRH+zkQEREJZsGdIFutEJ9qvs7NoGtyLABbswrweg0/BiYiItI03n//faZMmcIjjzzCunXr6NOnDyNGjODgwYO1bhMXF0dmZqbvsWfPniaMuGbWMvPLbYtdCbKIiDSe4E6QodpI1p2TYogIt1Ls8rDrcJF/4xIREWkCzz33HBMmTGD8+PH06NGDV199laioKObOnVvrNhaLhZSUFN8jOTm5CSOumdVdniA7lCCLiEjjCZ0E+ehubFYLPdrEAbBpf54fgxIREWl8LpeLtWvXMnz4cN8yq9XK8OHDWbFiRa3bFRYW0rFjR1JTU7niiiv48ccfT3gcp9NJfn5+tUdDs7nN8UNsjpgG37eIiEiF4E+Qk7qZz1mbAOjZLh6AHw80/M1bRESkOcnJycHj8RxXAU5OTiYrK6vGbbp27crcuXP55JNPeOedd/B6vZx77rns27ev1uPMnDmT+Ph43yM1NbVBzwPDIMxjJshhEUqQRUSk8QR/gtyuv/m8fw0YBj3bmgmyKsgiIiLHGzJkCLfeeit9+/blggsuYP78+SQlJfG3v/2t1m2mTZtGXl6e77F3796GDcpdihUvAGGRsQ27bxERkSrC/B1Ao2vTGyw2KMyG/P2c3a6yibVhGFgsFj8HKCIi0jhatWqFzWYjOzu72vLs7GxSUlLqtI/w8HD69evHzp07a13H4XDgcDhOK9YTclXOPmGPVAVZREQaT/BXkMMjIfls8/X+tXRpHYvdZiW/1M2+o5oPWUREgpfdbqd///6kp6f7lnm9XtLT0xkyZEid9uHxePjhhx9o06ZNY4V5cuVTPJUa4URH2P0Xh4iIBL3gT5ChSjPrtdjDrHRNMZtn/aBm1iIiEuSmTJnCnDlzePPNN9myZQt33HEHRUVFjB8/HoBbb72VadOm+dZ//PHH+fLLL/n5559Zt24dN998M3v27OG2227z1ylA+RRPxTiIdgR/4zcREfGf0LjLtOsPa1+H/esA6Nkujh/257Fpfx6X9vLjN+IiIiKN7IYbbuDQoUNMnz6drKws+vbty+eff+4buCsjIwOrtfL78qNHjzJhwgSysrJITEykf//+LF++nB49evjrFMBlTs1YTAQxSpBFRKQRhcZdpqKCfGA9eD2c3TYe2MsmjWQtIiIhYNKkSUyaNKnGz5YsWVLt/fPPP8/zzz/fBFHVQ0WCbDiItofGny4iIuIfodHEOqkrhEebfZgObauc6ql8oC4RERFpxnxNrCPUxFpERBpVaCTIVhu07We+3r+Wbimx2KwWDhe5yMov9W9sIiIicmJVKshqYi0iIo0pdO4y7fvDnqWwfy0R59xCl9YxbM0qYNP+fNrER/o7OhEREamNrw+ygxSHzc/BiEgo8Xg8lJWV+TsMqYPw8HBsttO/R4ROglxlJGuAs9vGlyfIefy6R7IfAxMREZETMVyFWNAgXSLSdAzDICsri9zcXH+HIvWQkJBASkoKFovllPcROneZigQ5+0coK6Fnuzj+vQ5+PKCpnkRERJqzspJC7JQP0qUEWUSaQEVy3Lp1a6Kiok4r4ZLGZxgGxcXFHDx4EIA2bU59pqLQucvEtYOYZCjMhszv6dmuC2DOhWwYhv7Ri4iINFNlJQVmgkwEUXY1sRaRxuXxeHzJccuWLf0djtRRZKTZbfbgwYO0bt36lJtbh8YgXQAWS7Vm1me3jcMeZiU738mPmu5JRESk2XKXmn2Q3bZIfaEtIo2uos9xVFSUnyOR+qr4mZ1Ov/HQSZAB2p1jPu9fQ5Q9jIvL+x5/uHafH4MSERGRE/E4CwFwh+mPVRFpOvpCLvA0xM8stBLkDkPM553p4HZyTf/2AHyyYT8ut9ePgYmIiEhtvE6zguwJ06wTIiLSuEIvQY5tC6W5sP0LzjuzFa1jHRwtLmPxtoP+jk5ERERqYJRP82SER/s5EhERCXahlSBbbdD7OvP1xnmE2axc1a8doGbWIiIizZXFlyCribWISFNJS0tj1qxZ/g6jyYVWggzQ+0bzeceXUHTY18x68daDHC50+jEwERERqYnFXWw+21VBFhGpzbBhw5g8eXKD7W/16tX8/ve/b7D9BYrQS5CTe0BKb/CWwY/zOSs5lt7t43F7DT7ZcMDf0YmIiMgxrGVmBdmqBFlE5LQYhoHb7a7TuklJSSE5knfoJcgAfUabzxvnAXDNOWYV+d/r1MxaRESkubG5SwCwRsT4ORIRCUWGYVDscvvlYRhGnWIcN24cX3/9NbNnz8ZisWCxWNi9ezdLlizBYrHw3//+l/79++NwOFi6dCk//fQTV1xxBcnJycTExDBw4ED+97//VdvnsU2sLRYLf//737nqqquIioqiS5cufPrppyeM6+2332bAgAHExsaSkpLCTTfdxMGD1cd++vHHH7n88suJi4sjNjaW8847j59++sn3+dy5czn77LNxOBy0adOGSZMm1emanKqwRt17c9XrWvjyIdi/BnJ28ps+HfjTgs38eCCftXuO0L9jC39HKCIiIuXCPWYT6zAlyCLiByVlHnpM/8Ivx978+Aii7CdP2WbPns327dvp2bMnjz/+OGBWgHfv3g3AAw88wDPPPEPnzp1JTExk7969XHrppTz55JM4HA7eeustRo0axbZt2+jQoUOtx3nsscf4y1/+wtNPP82LL77ImDFj2LNnDy1a1Jw/lZWV8cQTT9C1a1cOHjzIlClTGDduHAsXLgRg//79nH/++QwbNoyvvvqKuLg4li1b5qtyv/LKK0yZMoWnnnqKkSNHkpeXx7Jly+pzCestNBPkmNZw5kVmP+Tv55H4q4e4ql87/rVmH9M/+ZFPJg4lzBaaxXUREZHmJtxbCkBYRKyfIxERaZ7i4+Ox2+1ERUWRkpJy3OePP/44v/71r33vW7RoQZ8+fXzvn3jiCT766CM+/fTTE1Zox40bx+jRZmvcGTNm8MILL7Bq1SouueSSGtf/7W9/63vduXNnXnjhBQYOHEhhYSExMTG8/PLLxMfHM2/ePMLDwwE466yzfNv86U9/4t577+Xuu+/2LRs4cODJLsdpCc0EGaD3DWaCvOGf8Msp/PGSbny+KYsfD+Tzznd7GDe0k78jFBEREU8ZYUYZAOFRSpBFpOlFhtvY/PgIvx27IQwYMKDa+8LCQh599FEWLFhAZmYmbrebkpISMjIyTrif3r17+15HR0cTFxd3XJPpqtauXcujjz7Kxo0bOXr0KF6vF4CMjAx69OjBhg0bOO+883zJcVUHDx7kwIEDXHTRRfU51dMWuglyt8sgJgXy98GXD9Lq8uf54yXdeOjjTTz75XYu7dWG1nER/o5SREQktJVP8QQQEakm1iLS9CwWS52aOTdn0dHVBzm87777WLRoEc888wxnnnkmkZGRXHvttbhcrhPu59hE1mKx+JLeYxUVFTFixAhGjBjBu+++S1JSEhkZGYwYMcJ3nMjIyFqPdaLPGlPotiMOj4SrXjFfr5kLWz5j9KAO9GkfT4HTzYyFW/wbn4iIiECZ2f+4zLD57Y8lEZFAYLfb8Xg8dVp32bJljBs3jquuuopevXqRkpLi66/cULZu3crhw4d56qmnOO+88+jWrdtx1ebevXvz7bffUlZWdtz2sbGxpKWlkZ6e3qBxnUzoJsgAZ/wKzr3LfP3pJGwFB3jiyp5YLPDxhgP8b3O2f+MTEREJdeUV5BIcREcc3wRPRERMaWlprFy5kt27d5OTk1NrZRegS5cuzJ8/nw0bNrBx40ZuuummE65/Kjp06IDdbufFF1/k559/5tNPP+WJJ56ots6kSZPIz8/nxhtvZM2aNezYsYO3336bbdu2AfDoo4/y7LPP8sILL7Bjxw7WrVvHiy++2KBxHiu0E2SAXz0MbfpCyVGY/3t6J4Vx6y86AjDxvXWs+Omwf+MTEREJZeUJcjEOYhyB3cRRRKQx3XfffdhsNnr06OFrzlyb5557jsTERM4991xGjRrFiBEjOOeccxo0nqSkJN544w0++OADevTowVNPPcUzzzxTbZ2WLVvy1VdfUVhYyAUXXED//v2ZM2eOryn32LFjmTVrFn/96185++yzufzyy9mxY0eDxnksi1HXybUaSH5+PvHx8eTl5REXF9eUh65dzk742/lQVgSxbXD/6lF+v6EzX207RJTdxju3DeacDon+jlJERBpBs7wvBbgGvaa7l8Ebl/KTtw3O21fRo61+RiLSuEpLS9m1axedOnUiIkJjEgWSE/3s6npvUgUZoNWZcNM8SEyDgkzCPvl//N39fzyasox2ZXsYO3cl3+/L9XeUIiIioae8D3KJKsgiItIEdKep0Ol8+MNK+O5l+OYZrPtXM47VjHNAjhFH+msDyBo6nl//+nIsVn2vICfgKobCbCg8CCVHzOb7JbnmH3mGF7xuwAKOWHDElD/HmY+IOIhsAVEtwdZM/3u6XeXndBRKc81nZ4H5cBVCWal5jhUPw1v5wAIWK1gsYLWBNQws5c+2MLCGg81uvrbZy9+Hl38eXv4+rMry8ueKdSrWq9in1VZ+PKv5Got5Dpby54r3vmXHfl51nfLGNr5GN0b5eZU/H/ve6wHDYz77rkf5a8NT5b2nhvdec5lvP1WuoWFUPxbG8c9VVWskZFSP1+utjLHaMaqch+9nV9MxT6Di54ylhufyn4mlfL3j1qly3av+LGo6Zr+bIarFiWORgOZxFmEDioigjaNhpjsRERGpTTP9C9xPwiPgvHuh942w8T3YvRRj7ypaleVzg+UrWPEV2Ws7kHDe73EMGm8mNxJavB4zESzMhiM/w5FdcHQ35GaYj7y94MxvmGNFJEB0EsS0Np9j20B8+8pHdJL5sEed2v7LSszEvTSverJbkdBXvC4+bCb6xeXJvquwYc5PpCF0vVQJcpBzFecTCZQYDqJVQRYRkUamO01N4tvB+ffD+fdjcbvw7lnOjkVzSM1cRLIrA9Ifwr30WcLO/QMMvE1/nDUHriLI3QsFB6AoB4oOmc+luZVJoLessgJVayXMqF7BKysFd4mZTLqKfE39TiosEmKTzUpwRAJEJphTi1nLK52GB5yFlZVXZ775KM0z48UwYy/NhcMnGYggPKq8Al1ejQ6LNKuoYQ7z87IScJeazxVVXmcheJx1vbo1sJjnVHFujjjz2PYY84umispuReXWYqusBFatilarrpaBxw0eVw2vyx8VyyuWed2Vy73uys8qqrL+ZrFWVratYce8t1Vem2rVdGv59bJWqYDbKiuyvuprbdXZKpXw2mKylK9nOSaGqvuteM/JjlmD2qrcvkq0UV4hr/rZMf8Pq+6n+glUeWnRF5UhwFVSYCbIlggcYWrBJSIijUsJ8smE2bGeMYyuZwxjzfYMFr3/EjeWfUwnZzYsftJ8RCSYFb2EjtCuH6QOhrbn6A+3hlJWalYuC7OgINtMgnMzzMrt0d1wdI9Z4WxK9lhokQYtOpt91xM6mo/49hDXxkwYa0seTsbrMc/Xl+gfhMJD5nnn7Ye8fZC/32zC7XGaSXtZMZxKYddihYh4iEwsT3YTzYQ3MrHKo7zJd1QL831UC3DEm4lcc2YYxzQd9lRPuI5rrltLE+XjVE3QqiSOVZsKVyTDp/pvQER8ykoKAHBZI7Do/5SIiDQyJcj1MOCsDnS590ke/88NuDbO546w/9DDuqey0pe9CbYtMFe22KDjudD9N9D9cohr68/Qm47XayZt7lKzsldRQfK6K6u5vua8udUrvKV5ZoWzrMis1rqKzM/qWul0xJvV/4qmx9GtKiucEfFmn1aonsxUvK9aDbNU6bcaHmFWaMMiwB5d3l84trI62xisNjP26FZAt9rXMwzzehUdql4ZdpealVePy1wvLMKsXodFVFZ5HTHmtXHEBm8SZ7E0337cIlJn7lJzmie3LdLPkYiISCjQX4/1FB8VzrM39Od/vdoz7qMLKS44ShvLEXrH5HNjZyf9rDsI278G8vfB7m/Nx3/vN+daPnM4nHkRtB9oNj8NRF4P5B8wm/3m7ITDOyF3j9m8uSH73x7LYoWYZPMRmwIJHczKra96m2omwaHEYjEH9YrQlCciErw8pWbzGLftFMdbEBERqQclyKdoeI9kfnFGS979bg9zvt3Fvwuc/HsjJEQN4qZB0xjfHZL2L4It/4G9KyFzg/n49hmzktnyTEjqComdzGpeeLQ52JI9uvK121nZRzUiDpJ7mk16rU0wiqezELJ+KH9shJwdZvPegsz69e2sqNRawyqb8lbtuxoRb76OiC/vyxpb5RpEV65rj2n+TXpFRKTBeV1mBdkbrgRZREQanxLk0xDjCOP/XXAGY89N44M1e5nz7S4yjhTz1yU/8do3Fkb0HMpNF4xmSGs31p8Xw0/p8NNX5qjABzebj/oKi4Dks6HDEOjwC2g3wEw6wxxmRdHrBWf5QE8Vg0q5iszKr7XKYEA2B4SVT6PjLDCbOhcfhgMbIGOFmRjXlghbw8zEvlUXM9GvWsWNamnGGBah5q0iInLaDKdZQVaCLCIiTUEZTAOICLdxy5A0bhrckUWbs5m7dBerdh9hwfeZLPg+k44to7hx4CCuvfhqkqLDzabIOdvh0FZzsClXcXm/2/LBllyF5uswR+XoxEWHIHuzOaLy/rXmY8VLlUFYrOboxWXFnHyQoTqKbQttekNKb2jd3WzWHNfOnHaoKarYIiIS8iwVsweERfs3EBGRZm7YsGH07duXWbNmNdg+x40bR25uLh9//HGD7bO5U4LcgGxWC5f0TOGSnin8eCCPeav28vH6/ew5XMyfP9/Ks19u4+Kzk7lxYAeGnjkcW5df1+8AXo857+6BdbBnOWR8B4e2mJ8ZXjPJrhAeZTZLtkeZzZWttvJpddyVU+Z4nOZzxYBNEfFms++K6nR8+wa7NiIiIqekIkF2KEEWEZHGpwS5kZzdNp4nroxn2qXd+GxjJv9cncH6jFwW/pDFwh+yaJcQyTX923PtOe3p0LKOzcasNmh1pvnofb25zOM2E+OyEvOPCHuMmeg25ijLIiIiTcRW/uWv1a4EWUSkNuPGjePrr7/m66+/Zvbs2QDs2rWLtLQ0Nm3axP3338+3335LdHQ0F198Mc8//zytWrUC4MMPP+Sxxx5j586dREVF0a9fPz755BOefvpp3nzzTQDfNHuLFy9m2LBhxx3/888/509/+hObNm3CZrMxZMgQZs+ezRlnnOFbZ9++fdx///188cUXOJ1Ounfvzssvv8zgwYMB+M9//sPjjz/ODz/8QExMDOeddx4fffRRY162GilBbmRR9jCuH5jK9QNT2ZKZz7xVGXy84QD7c0t4IX0HL6TvoFtKLL/q1poLu7WmV7t4IsLr0XzZFga2+NAbwVlEREKCzV0CgDUixs+RiEjIMozK1ixNLTyqTlNyzp49m+3bt9OzZ08ef/xxAJKSksjNzeVXv/oVt912G88//zwlJSVMnTqV66+/nq+++orMzExGjx7NX/7yF6666ioKCgr49ttvMQyD++67jy1btpCfn8/rr78OQIsWLWo8flFREVOmTKF3794UFhYyffp0rrrqKjZs2IDVaqWwsJALLriAdu3a8emnn5KSksK6devwer0ALFiwgKuuuooHH3yQt956C5fLxcKFCxvoItaPEuQm1L1NHI9d0ZNpl3bny83ZfLBmL8t25rA1q4CtWQX8dclP2KwWzkyK4ey2cQzs1ILzz0qiXYLmfhQRkVP38ssv8/TTT5OVlUWfPn148cUXGTRo0Em3mzdvHqNHj+aKK67wW/+zMI/5R6nNoQRZRPykrBhmtPXPsf/vgDmzy0nEx8djt9uJiooiJSXFt/yll16iX79+zJgxw7ds7ty5pKamsn37dgoLC3G73Vx99dV07NgRgF69evnWjYyMxOl0VttnTa655ppq7+fOnUtSUhKbN2+mZ8+evPfeexw6dIjVq1f7kuwzzzzTt/6TTz7JjTfeyGOPPeZb1qdPn5Oed2NQguwHEeE2ftOnLb/p05ajRS6+2XGI9C0HWbYzh8NFLrZlF7Atu4D56/cD0Dkpml90bknvdvH0bBdP15RYwm2a8khERE7u/fffZ8qUKbz66qsMHjyYWbNmMWLECLZt20br1q1r3W737t3cd999nHfeeU0Y7fHCvaUA2COVIIuI1NfGjRtZvHgxMTHH/w796aefuPjii7nooovo1asXI0aM4OKLL+baa68lMTGxXsfZsWMH06dPZ+XKleTk5PgqwxkZGfTs2ZMNGzbQr1+/WivQGzZsYMKECfU/wUagBNnPEqPtXNG3HVf0bYdhGGTll/Lj/ny+35/Hsp05bNiby8+Hivj5UBHvlW9jD7PSvU1cecIcxxlJMaS1iqZltN3XP0BERATgueeeY8KECYwfPx6AV199lQULFjB37lweeOCBGrfxeDyMGTOGxx57jG+//Zbc3NwmjLg6h9dsYh0eGeu3GEQkxIVHmZVcfx37NBQWFjJq1Cj+/Oc/H/dZmzZtsNlsLFq0iOXLl/Pll1/y4osv8uCDD7Jy5Uo6depU5+OMGjWKjh07MmfOHNq2bYvX66Vnz564XC7ArESfyMk+b0pKkJsRi8VCm/hI2sRHMrxHMlN+fRZ5JWWs+Okw6/ceZdP+PL7fl0dBqZuNe3PZuDe32vaxEWG0jY8kOT6ClDgHSbEOWsWYj5T4CDq0iCIpxoHVqiRaRCQUuFwu1q5dy7Rp03zLrFYrw4cPZ8WKFbVu9/jjj9O6dWt+97vf8e233570OE6nE6fT6Xufn59/eoFXMAzW2ftTVpyPPSahYfYpIlJfFkudmjn7m91ux+PxVFt2zjnn8O9//5u0tDTCwmpO/SwWC0OHDmXo0KFMnz6djh078tFHHzFlypQa93msw4cPs23bNubMmeNrdbR06dJq6/Tu3Zu///3vHDlypMYqcu/evUlPT/d9metPp5Qgn2pfJqm/+Mhw39RRAIZhkHGkmO/35bFpfx6bM/P5+VARB/JKKCh1s63UbJ5dG0eYlXaJkbSOddA6NoLWsQ4So+20iLaTGBVOtCOMiHAbjjAr0Y4w4iLCiY8Mxx6mJt0iIoEmJycHj8dDcnJyteXJycls3bq1xm2WLl3KP/7xDzZs2FDn48ycObNav7EGY7HwWOQDbM0r4O3YVg2/fxGRIJKWlsbKlSvZvXs3MTExtGjRgokTJzJnzhxGjx7NH//4R1q0aMHOnTuZN28ef//731mzZg3p6elcfPHFtG7dmpUrV3Lo0CG6d+/u2+cXX3zBtm3baNmyJfHx8YSHh1c7bmJiIi1btuS1116jTZs2ZGRkHNdCafTo0cyYMYMrr7ySmTNn0qZNG9avX0/btm0ZMmQIjzzyCBdddBFnnHEGN954I263m4ULFzJ16tQmu34V6p0gn2pfJmkYFouFji2j6dgymlF9KgcLKC3zkHGkmMy8UrLzS8nOK+VQoZOcQic5BS4y80s4kFuK0+31Ndmuj4hwKzGOMKIdYUTbw4iNqHiE+5bHOGw4wmxYrRbCrBbCbVYi7VYiw8OIstuwh1nNh818rljHarVQUdO2WS3YrBbCrVbCbObn4TaLmo6LiDSBgoICbrnlFubMmeOb/qMupk2bxpQpU3zv8/PzSU1NbZCYCp1uAKIdavQmInIi9913H2PHjqVHjx6UlJT4pnlatmwZU6dO5eKLL8bpdNKxY0cuueQSrFYrcXFxfPPNN8yaNYv8/Hw6duzIs88+y8iRIwGYMGECS5YsYcCAARQWFtY4zZPVamXevHncdddd9OzZk65du/LCCy9UW89ut/Pll19y7733cumll+J2u+nRowcvv/wyAMOGDeODDz7giSee4KmnniIuLo7zzz+/qS5dNRbDMIz6bDB48GAGDhzISy+9BIDX6yU1NZU777yzxr5MNTW7Sk1NJS8vj7i4uNMMX+qjzOMlM7eUfbnFHCpw+h5HilwcLXZxpMhFscuD0+2ltMxDodNNQanb32EDYLdZsVrBarFgs1gwAK9h4PEaWC1mQm6zWbBaKpNti8X8QsFmMZNuyrfxGgYWLFjLPzfXAwvma98+LFA1La9I0mtL1avu49htTrRdxbZVn+vi2GPVxDDAwKDif3nVGKtteoIdne5XE6f73UZjfjVS8fM50a/BUP1yJlDOevbofqc90n9+fj7x8fFBeV9yuVxERUXx4YcfcuWVV/qWjx07ltzcXD755JNq61cMomKzVU43WDHQitVqZdu2bdXmtKxNQ17Tfo9/ydHiMr6853zOSlY/ZBFpfKWlpezatYtOnToRERHh73CkHk70s6vrvaleX8eeSl+mRmt2JfUWbrPSoWUUHVrWvbO/x2tQUFpGQambQqebIqebAqebwlIzeS4oLaPI6abQ6aHI6cbl8eLxmomry+OlxOWh2OWmpMyLy+3B5fHicnsp8xiUeby4PQYeM4vDwNzOW0Ou4vJ44cTdH0QkBJWW6RfDidjtdvr37096erovQfZ6vaSnpzNp0qTj1u/WrRs//PBDtWUPPfQQBQUFzJ49u8GqwvVR5DR/xqogi4hIU6jX3eZU+jI1ZrMraXw2q4WEKDsJUfYmO6ZRXhku85hJdll5Uu3xmpVQj2FgKY/NYjGrpB6vgdtrVocreA0Dr7ey0nxshddrmPvzGgYGlFdZK5ZVVhUrP6sSI2YVuup7w6i+nrnXinM6wflWOe9jV6utilcequ+4vgp0TVtUqRZXbOetY8ORuqxmnCxO376MWqux9WzIctx1ChT1PM1GUZeC+OnHWdu/ipPt+GTBHb/f5Dh9s38yU6ZMYezYsQwYMIBBgwYxa9YsioqKfAOh3HrrrbRr146ZM2cSERFBz549q22fkJAAcNzypmAYBq/ecg6FTg8to5vuPiQiIqGr0b+OdTgcOByOxj6MBBGLxUKYzUKYDSKxnXwDERGp1Q033MChQ4eYPn06WVlZ9O3bl88//9z3ZXdGRgZWa/MciNFisfCrbsknX1FERKSB1CtBbtWqFTabjezs7GrLs7OzSUlJadDAREREpGFMmjSpxibVAEuWLDnhtm+88UbDByQiItJM1esr46p9mSpU9GUaMmRIgwcnIiIiIiIi0lTq3cT6ZH2ZREREREREAl3FKP4SOBriZ1bvBPlkfZlEREREREQCld1ux2q1cuDAAZKSkrDb7SE77WOgMAwDl8vFoUOHsFqt2O2nPrBjvedBPl3BPN+kiIgEHt2XGp6uqYgEOpfLRWZmJsXFxf4OReohKiqKNm3a1JggN8o8yCIiIiIiIsHObrfToUMH3G43Ho/H3+FIHdhsNsLCwk672q8EWURERERE5BgWi4Xw8HDCw8P9HYo0oeY58aGIiIiIiIhIE1OCLCIiIiIiIoISZBERERERERHAD32QKwbNzs/Pb+pDi4iIHKfiftTEkzoENd3rRUSkuanr/b7JE+SCggIAUlNTm/rQIiIitSooKCA+Pt7fYQQF3etFRKS5Otn9vsnnQfZ6vRw4cIDY2NjTHoI7Pz+f1NRU9u7dG5LzLIb6+YOuAegahPr5g67B6Z6/YRgUFBTQtm1brFb1PGoIutc3rFC/BqF+/qBrEOrnD7oG0HT3+yavIFutVtq3b9+g+4yLiwvZfyig8wddA9A1CPXzB12D0zl/VY4blu71jSPUr0Gonz/oGoT6+YOuATT+/V5flYuIiIiIiIigBFlEREREREQECPAE2eFw8Mgjj+BwOPwdil+E+vmDrgHoGoT6+YOuQaiff7DTz1fXINTPH3QNQv38QdcAmu4aNPkgXSIiIiIiIiLNUUBXkEVEREREREQaihJkEREREREREZQgi4iIiIiIiABKkEVEREREREQAJcgiIiIiIiIiQAAnyC+//DJpaWlEREQwePBgVq1a5e+QGs3MmTMZOHAgsbGxtG7dmiuvvJJt27ZVW6e0tJSJEyfSsmVLYmJiuOaaa8jOzvZTxI3rqaeewmKxMHnyZN+yUDj//fv3c/PNN9OyZUsiIyPp1asXa9as8X1uGAbTp0+nTZs2REZGMnz4cHbs2OHHiBuOx+Ph4YcfplOnTkRGRnLGGWfwxBNPUHUQ/mA7/2+++YZRo0bRtm1bLBYLH3/8cbXP63K+R44cYcyYMcTFxZGQkMDvfvc7CgsLm/AsTs+JrkFZWRlTp06lV69eREdH07ZtW2699VYOHDhQbR+Bfg0kdO73utdXp3t96N3rIfTu97rXN9N7vRGA5s2bZ9jtdmPu3LnGjz/+aEyYMMFISEgwsrOz/R1aoxgxYoTx+uuvG5s2bTI2bNhgXHrppUaHDh2MwsJC3zq33367kZqaaqSnpxtr1qwxfvGLXxjnnnuuH6NuHKtWrTLS0tKM3r17G3fffbdvebCf/5EjR4yOHTsa48aNM1auXGn8/PPPxhdffGHs3LnTt85TTz1lxMfHGx9//LGxceNG4ze/+Y3RqVMno6SkxI+RN4wnn3zSaNmypfHZZ58Zu3btMj744AMjJibGmD17tm+dYDv/hQsXGg8++KAxf/58AzA++uijap/X5XwvueQSo0+fPsZ3331nfPvtt8aZZ55pjB49uonP5NSd6Brk5uYaw4cPN95//31j69atxooVK4xBgwYZ/fv3r7aPQL8GoS6U7ve611fSvT407/WGEXr3e93rm+e9PiAT5EGDBhkTJ070vfd4PEbbtm2NmTNn+jGqpnPw4EEDML7++mvDMMx/POHh4cYHH3zgW2fLli0GYKxYscJfYTa4goICo0uXLsaiRYuMCy64wHfTDIXznzp1qvHLX/6y1s+9Xq+RkpJiPP30075lubm5hsPhMP75z382RYiN6rLLLjN++9vfVlt29dVXG2PGjDEMI/jP/9gbRl3Od/PmzQZgrF692rfOf//7X8NisRj79+9vstgbSk1/OBxr1apVBmDs2bPHMIzguwahKJTv97rX615/rGC/1xlGaN/vda9vPvf6gGti7XK5WLt2LcOHD/cts1qtDB8+nBUrVvgxsqaTl5cHQIsWLQBYu3YtZWVl1a5Jt27d6NChQ1Bdk4kTJ3LZZZdVO08IjfP/9NNPGTBgANdddx2tW7emX79+zJkzx/f5rl27yMrKqnYN4uPjGTx4cFBcg3PPPZf09HS2b98OwMaNG1m6dCkjR44Egv/8j1WX812xYgUJCQkMGDDAt87w4cOxWq2sXLmyyWNuCnl5eVgsFhISEoDQvAbBJNTv97rX614favd60P2+Kt3ra9YU9/qwhgi0KeXk5ODxeEhOTq62PDk5ma1bt/opqqbj9XqZPHkyQ4cOpWfPngBkZWVht9t9/1AqJCcnk5WV5YcoG968efNYt24dq1evPu6zUDj/n3/+mVdeeYUpU6bwf//3f6xevZq77roLu93O2LFjfedZ0/+LYLgGDzzwAPn5+XTr1g2bzYbH4+HJJ59kzJgxAEF//seqy/lmZWXRunXrap+HhYXRokWLoLwmpaWlTJ06ldGjRxMXFweE3jUINqF8v9e9Xvf6ULzXg+73Velef7ymutcHXIIc6iZOnMimTZtYunSpv0NpMnv37uXuu+9m0aJFRERE+Dscv/B6vQwYMIAZM2YA0K9fPzZt2sSrr77K2LFj/Rxd4/vXv/7Fu+++y3vvvcfZZ5/Nhg0bmDx5Mm3btg2J85cTKysr4/rrr8cwDF555RV/hyNy2nSv170eQu9eD7rfS+2a8l4fcE2sW7Vqhc1mO27UwuzsbFJSUvwUVdOYNGkSn332GYsXL6Z9+/a+5SkpKbhcLnJzc6utHyzXZO3atRw8eJBzzjmHsLAwwsLC+Prrr3nhhRcICwsjOTk5qM8foE2bNvTo0aPasu7du5ORkQHgO89g/X9x//3388ADD3DjjTfSq1cvbrnlFu655x5mzpwJBP/5H6su55uSksLBgwerfe52uzly5EhQXZOKG+aePXtYtGiR7xtlCJ1rEKxC9X6ve73u9VWF0r0edL+vSvf6Sk19rw+4BNlut9O/f3/S09N9y7xeL+np6QwZMsSPkTUewzCYNGkSH330EV999RWdOnWq9nn//v0JDw+vdk22bdtGRkZGUFyTiy66iB9++IENGzb4HgMGDGDMmDG+18F8/gBDhw49brqP7du307FjRwA6depESkpKtWuQn5/PypUrg+IaFBcXY7VW/3Vls9nwer1A8J//sepyvkOGDCE3N5e1a9f61vnqq6/wer0MHjy4yWNuDBU3zB07dvC///2Pli1bVvs8FK5BMAu1+73u9brXh/q9HnS/r0r3epNf7vWnNLSXn82bN89wOBzGG2+8YWzevNn4/e9/byQkJBhZWVn+Dq1R3HHHHUZ8fLyxZMkSIzMz0/coLi72rXP77bcbHTp0ML766itjzZo1xpAhQ4whQ4b4MerGVXVkS8MI/vNftWqVERYWZjz55JPGjh07jHfffdeIiooy3nnnHd86Tz31lJGQkGB88sknxvfff29cccUVATvtwbHGjh1rtGvXzjftw/z5841WrVoZf/zjH33rBNv5FxQUGOvXrzfWr19vAMZzzz1nrF+/3jdqY13O95JLLjH69etnrFy50li6dKnRpUuXgJr64UTXwOVyGb/5zW+M9u3bGxs2bKj2u9HpdPr2EejXINSF0v1e9/rj6V4fWvd6wwi9+73u9c3zXh+QCbJhGMaLL75odOjQwbDb7cagQYOM7777zt8hNRqgxsfrr7/uW6ekpMT4wx/+YCQmJhpRUVHGVVddZWRmZvov6EZ27E0zFM7/P//5j9GzZ0/D4XAY3bp1M1577bVqn3u9XuPhhx82kpOTDYfDYVx00UXGtm3b/BRtw8rPzzfuvvtuo0OHDkZERITRuXNn48EHH6z2yzHYzn/x4sU1/r8fO3asYRh1O9/Dhw8bo0ePNmJiYoy4uDhj/PjxRkFBgR/O5tSc6Brs2rWr1t+Nixcv9u0j0K+BhM79Xvf64+leH1r3esMIvfu97vXN815vMQzDOLXas4iIiIiIiEjwCLg+yCIiIiIiIiKNQQmyiIiIiIiICEqQRURERERERAAlyCIiIiIiIiKAEmQRERERERERQAmyiIiIiIiICKAEWURERERERARQgiwiIiIiIiICKEEWERERERERAZQgi4iIiIiIiABKkEVEREREREQA+P9luYWouPuGWQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label=\"train loss\")\n",
    "plt.plot(test_loss, label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_acc, label=\"train acc\")\n",
    "plt.plot(test_acc, label=\"test acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy sur les données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision sur l'ensemble de test : 89.67%\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle entraîné s'il n'a pas déjà été chargé\n",
    "model = SpeechRecognition(hidden_size=hidden_size, num_classes=num_classes, n_feats=input_size, num_layers=1, dropout=0.1)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# Mettre le modèle en mode évaluation\n",
    "model.eval()\n",
    "\n",
    "# Calculer la précision sur l'ensemble de donnée de test\n",
    "accuracy_test = accuracy(test_loader, model)\n",
    "print(f\"Précision sur l'ensemble de test : {100*accuracy_test:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation et création du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'height', 'nine']\n"
     ]
    }
   ],
   "source": [
    "# Chargement des labels\n",
    "with open(\"dataset/audio_path.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    transcriptions = json.load(f)\n",
    "\n",
    "labels = np.array([transcription[\"transcription\"] for transcription in transcriptions])\n",
    "data = labels\n",
    "\n",
    "vocab = []\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "for sentence in data:\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token not in word_to_index:\n",
    "            word_to_index[token] = len(word_to_index)\n",
    "            index_to_word[len(index_to_word)] = token\n",
    "            vocab.append(token)\n",
    "            \n",
    "print(vocab)\n",
    "# Encodage des séquences\n",
    "encoded_data = []\n",
    "for sentence in data:\n",
    "    tokens = sentence.split()\n",
    "    encoded_sentence = [word_to_index[token] for token in tokens]\n",
    "    encoded_data.append(encoded_sentence)\n",
    "\n",
    "# Création du Dataset et DataLoader\n",
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index])\n",
    "    \n",
    "dataset = LanguageDataset(encoded_data)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Décodage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 494)\n",
      "torch.Size([1, 1, 3000])\n",
      "Texte transcrit : nine\n"
     ]
    }
   ],
   "source": [
    "X = np.load(\"works.npy\")\n",
    "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "print(X.shape)\n",
    "X_tensor = torch.Tensor(X)\n",
    "# Charger le modèle entraîné\n",
    "model = SpeechRecognition(hidden_size=hidden_size, num_classes=num_classes, n_feats=input_size, num_layers=1, dropout=0.1)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "def decode_output(output, vocab):\n",
    "    decoded_sequence = []\n",
    "    print(output.shape)\n",
    "    for timestep_output in output:\n",
    "        predicted_symbol_index = torch.argmax(timestep_output).item()\n",
    "        predicted_symbol = vocab[predicted_symbol_index]\n",
    "        decoded_sequence.append(predicted_symbol)\n",
    "    return ' '.join(decoded_sequence)\n",
    "\n",
    "# Utiliser le modèle pour transcrire l'audio\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs, _ = model(X_tensor.unsqueeze(0), model._init_hidden(X_tensor.size(0)))\n",
    "    decoded_text = decode_output(outputs, vocab)\n",
    "    print(\"Texte transcrit :\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4637fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte transcrit : zero nine two five seven three four nine zero one zero five nine height one three zero one height five height two nine seven six height two height four six four nine five height height four nine one seven nine five four nine two three nine seven nine four two six six height one zero one zero three zero one zero four nine six six six six one four three height four seven four three one two three height six two five one seven zero one six nine two nine six two nine seven zero five four two six five three seven height one three six four seven four two nine six four five one six six nine seven six one six two four height five four six five zero three five zero zero six height two height two one six three six seven seven height one seven six nine five four zero zero seven three four height two three height six height two three height five zero six two six height four nine nine four nine three height five three three zero seven four five nine seven six three five five nine four three seven zero four three five four five seven one seven two five three two height four five two three five zero height three two one one four six nine five three one two zero five three seven one four five six three two one zero height five nine two two seven five three height six nine six two height two zero six six height zero three seven two six one one six zero two zero one two five three height one five one two five nine nine zero three five six zero three height seven two zero seven five seven zero nine three four one\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle entraîné\n",
    "model = SpeechRecognition(hidden_size=hidden_size, num_classes=num_classes, n_feats=input_size, num_layers=1, dropout=0.1)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Définir le décodeur\n",
    "def decode_output(output, vocab):\n",
    "    decoded_sequence = []\n",
    "    for timestep_output in output:\n",
    "        predicted_symbol_index = torch.argmax(timestep_output).item()\n",
    "        predicted_symbol = vocab[predicted_symbol_index]\n",
    "        decoded_sequence.append(predicted_symbol)\n",
    "    return ' '.join(decoded_sequence)\n",
    "\n",
    "# Utiliser le modèle pour transcrire l'audio\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs, _ = model(inputs.unsqueeze(1), model._init_hidden(inputs.size(0)))\n",
    "        decoded_text = decode_output(outputs.squeeze(), vocab)\n",
    "        print(\"Texte transcrit :\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a0014",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1, 494)\n",
      "Prédictions: tensor([[ 271,  496,  697,  ..., 1578, 1544, 1146]])\n"
     ]
    }
   ],
   "source": [
    "# 1. Charger le modèle\n",
    "model = SpeechRecognition(hidden_size=hidden_size, num_classes=num_classes, n_feats=input_size, num_layers=1, dropout=0.1)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# 2. Prétraiter les données audio\n",
    "X = np.load(\"mfcc_feature1.npy\")\n",
    "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "print(X.shape)\n",
    "X_tensor = torch.Tensor(X)\n",
    "\n",
    "# Si vous utilisez GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "X_tensor = X_tensor.to(device)\n",
    "\n",
    "# Obtenir les prédictions\n",
    "with torch.no_grad():\n",
    "    outputs, _ = model(X_tensor.unsqueeze(1), model._init_hidden(X_tensor.size(0)))\n",
    "\n",
    "# Appliquer la fonction softmax si nécessaire\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "predictions = torch.argmax(probs, dim=1)\n",
    "\n",
    "# Faire quelque chose avec les prédictions\n",
    "print(\"Prédictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hubs/.local/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.001 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = LSTMWakeWord(num_classes, input_size, hidden_size, 1, 0.001, False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c38fb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 1 1 1 0]\n",
      " [1 0 0 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Exemple de données texte\n",
    "texts = [\"Ceci est un exemple de phrase.\", \"Voici un autre exemple.\"]\n",
    "\n",
    "# Initialiser le CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Adapter le vectorizer aux données et encoder les données texte\n",
    "encoded_texts = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Afficher les textes encodés\n",
    "print(encoded_texts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mixed dtype (CPU): all inputs must share same datatype.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Évaluation du modèle (exemples de prédiction)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m input_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[word_to_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m\"\u001b[39m], word_to_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthree\u001b[39m\u001b[38;5;124m\"\u001b[39m], word_to_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthree\u001b[39m\u001b[38;5;124m\"\u001b[39m], word_to_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthree\u001b[39m\u001b[38;5;124m\"\u001b[39m], word_to_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m\"\u001b[39m]]])\n\u001b[0;32m----> 3\u001b[0m output_test \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m predicted_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output_test)\n\u001b[1;32m      5\u001b[0m predicted_word \u001b[38;5;241m=\u001b[39m index_to_word[predicted_index\u001b[38;5;241m.\u001b[39mitem()]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 23\u001b[0m, in \u001b[0;36mLSTMWakeWord.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# x.shape => seq_len, batch, feature\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_hidden(x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     25\u001b[0m     out, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x, hidden)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/functional.py:2573\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2571\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2572\u001b[0m     )\n\u001b[0;32m-> 2573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mixed dtype (CPU): all inputs must share same datatype."
     ]
    }
   ],
   "source": [
    "# Évaluation du modèle (exemples de prédiction)\n",
    "input_test = torch.tensor([[word_to_index[\"one\"], word_to_index[\"three\"], word_to_index[\"three\"], word_to_index[\"three\"], word_to_index[\"one\"]]])\n",
    "output_test = model(input_test)\n",
    "predicted_index = torch.argmax(output_test)\n",
    "predicted_word = index_to_word[predicted_index.item()]\n",
    "print('Predicted Word:', predicted_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcess:\n",
    "\tdef __init__(self):\n",
    "\t\tchar_map_str = \"\"\"\n",
    "\t\t' 0\n",
    "\t\t<SPACE> 1\n",
    "\t\ta 2\n",
    "\t\tb 3\n",
    "\t\tc 4\n",
    "\t\td 5\n",
    "\t\te 6\n",
    "\t\tf 7\n",
    "\t\tg 8\n",
    "\t\th 9\n",
    "\t\ti 10\n",
    "\t\tj 11\n",
    "\t\tk 12\n",
    "\t\tl 13\n",
    "\t\tm 14\n",
    "\t\tn 15\n",
    "\t\to 16\n",
    "\t\tp 17\n",
    "\t\tq 18\n",
    "\t\tr 19\n",
    "\t\ts 20\n",
    "\t\tt 21\n",
    "\t\tu 22\n",
    "\t\tv 23\n",
    "\t\tw 24\n",
    "\t\tx 25\n",
    "\t\ty 26\n",
    "\t\tz 27\n",
    "\t\t\"\"\"\n",
    "\t\tself.char_map = {}\n",
    "\t\tself.index_map = {}\n",
    "\t\tfor line in char_map_str.strip().split('\\n'):\n",
    "\t\t\tch, index = line.split()\n",
    "\t\t\tself.char_map[ch] = int(index)\n",
    "\t\t\tself.index_map[int(index)] = ch\n",
    "\t\tself.index_map[1] = ' '\n",
    "\n",
    "\tdef text_to_int_sequence(self, text):\n",
    "\t\t\"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "\t\tint_sequence = []\n",
    "\t\tfor c in text:\n",
    "\t\t\tif c == ' ':\n",
    "\t\t\t\tch = self.char_map['<SPACE>']\n",
    "\t\t\telse:\n",
    "\t\t\t\tch = self.char_map[c]\n",
    "\t\t\tint_sequence.append(ch)\n",
    "\t\treturn int_sequence\n",
    "\n",
    "\tdef int_to_text_sequence(self, labels):\n",
    "\t\t\"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "\t\tstring = []\n",
    "\t\tfor i in labels:\n",
    "\t\t\tstring.append(self.index_map[i])\n",
    "\t\treturn ''.join(string).replace('<SPACE>', ' ')\n",
    "\n",
    "\n",
    "textprocess = TextProcess()\n",
    "\n",
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "\targ_maxes = torch.argmax(output, dim=2)\n",
    "\tdecodes = []\n",
    "\ttargets = []\n",
    "\tfor i, args in enumerate(arg_maxes):\n",
    "\t\tdecode = []\n",
    "\t\ttargets.append(textprocess.int_to_text_sequence(\n",
    "\t\t\t\tlabels[i][:label_lengths[i]].tolist()))\n",
    "\t\tfor j, index in enumerate(args):\n",
    "\t\t\tif index != blank_label:\n",
    "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tdecode.append(index.item())\n",
    "\t\tdecodes.append(textprocess.int_to_text_sequence(decode))\n",
    "\treturn decodes, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"'\",  # 0\n",
    "    \" \",  # 1\n",
    "    \"a\",  # 2\n",
    "    \"b\",\n",
    "    \"c\",\n",
    "    \"d\",\n",
    "    \"e\",\n",
    "    \"f\",\n",
    "    \"g\",\n",
    "    \"h\",\n",
    "    \"i\",\n",
    "    \"j\",\n",
    "    \"k\",\n",
    "    \"l\",\n",
    "    \"m\",\n",
    "    \"n\",\n",
    "    \"o\",\n",
    "    \"p\",\n",
    "    \"q\",\n",
    "    \"r\",\n",
    "    \"s\",\n",
    "    \"t\",\n",
    "    \"u\",\n",
    "    \"v\",\n",
    "    \"w\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"z\",  # 27\n",
    "    \"_\",  # 28, blank\n",
    "]\n",
    "\n",
    "def DecodeGreedy(output, blank_label=28, collapse_repeated=True):\n",
    "\targ_maxes = torch.argmax(output, dim=2).squeeze(1)\n",
    "\tdecode = []\n",
    "\tfor i, index in enumerate(arg_maxes):\n",
    "\t\tif index != blank_label:\n",
    "\t\t\tif collapse_repeated and i != 0 and index == arg_maxes[i -1]:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdecode.append(index.item())\n",
    "\treturn textprocess.int_to_text_sequence(decode)\n",
    "\n",
    "class CTCBeamDecoder:\n",
    "\n",
    "    def __init__(self, beam_size=100, blank_id=labels.index('_'), kenlm_path=None):\n",
    "        print(\"loading beam search with lm...\")\n",
    "        self.decoder = ctcdecode.CTCBeamDecoder(\n",
    "            labels, alpha=0.522729216841, beta=0.96506699808,\n",
    "            beam_width=beam_size, blank_id=labels.index('_'),\n",
    "            model_path=kenlm_path)\n",
    "        print(\"finished loading beam search\")\n",
    "\n",
    "    def __call__(self, output):\n",
    "        beam_result, beam_scores, timesteps, out_seq_len = self.decoder.decode(output)\n",
    "        return self.convert_to_string(beam_result[0][0], labels, out_seq_len[0][0])\n",
    "\n",
    "    def convert_to_string(self, tokens, vocab, seq_len):\n",
    "        return ''.join([vocab[x] for x in tokens[0:seq_len]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b302cdd1e032ee910f5c889c3360c28564c92ad4f326fc3102e39fbe47faee66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
