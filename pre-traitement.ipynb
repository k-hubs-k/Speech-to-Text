{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23811/3577002330.py:7: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  audio_lengths = [librosa.get_duration(filename=file) for file in tqdm(audio_files)]\n",
      "100%|██████████| 3000/3000 [02:57<00:00, 16.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4374343333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger les fichiers audio et calculer leurs longueurs\n",
    "audio_files = []\n",
    "for filename in os.listdir(\"audio_chiffre\"):\n",
    "    audio_path = os.path.join(\"audio_chiffre\", filename)\n",
    "    audio_files.append(audio_path)\n",
    "\n",
    "audio_lengths = [librosa.get_duration(filename=file) for file in tqdm(audio_files)]\n",
    "\n",
    "# Déterminer la longueur cible (par exemple, la plus grande longueur)\n",
    "target_length = np.mean(audio_lengths)\n",
    "\n",
    "output_file = \"mean_length.json\"\n",
    "\n",
    "# Ecriture du fichier json\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(target_length, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "print(target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Élimination des silences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence(audio_path, output):\n",
    "    # Charger le fichier audio\n",
    "    signal, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "    # Détection des régions actives dans le signal\n",
    "    non_silent_intervals = librosa.effects.split(signal, top_db=30)\n",
    "\n",
    "    # Fusionner les intervalles non silencieux\n",
    "    non_silent_signal = librosa.effects.remix(signal, non_silent_intervals)\n",
    "\n",
    "    # Sauvegarder le signal audio sans les silences\n",
    "    sf.write(output, non_silent_signal, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation du volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_audio_volume(audio_path, output_path, target_dBFS=-20.0):\n",
    "    # Chargement de l'enregistrement audio\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "    # Calcul du facteur de normalisation pour atteindre le niveau cible\n",
    "    current_dBFS = audio.dBFS\n",
    "    normalization_factor = (target_dBFS - current_dBFS)\n",
    "\n",
    "    # Normalisation du volume de l'audio\n",
    "    normalized_audio = audio + normalization_factor\n",
    "\n",
    "    # Export de l'audio normalisé\n",
    "    normalized_audio.export(output_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrage du bruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrage_du_bruit(audio_path, output, noise_threshold=-40.0):\n",
    "    # Chargement de l'enregistrement audio\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "    # Détection du bruit de fond\n",
    "    background_noise = audio.dBFS\n",
    "\n",
    "    # Filtrer le bruit de fond\n",
    "    if background_noise > noise_threshold:\n",
    "        audio = audio - noise_threshold\n",
    "    else:\n",
    "        audio = audio - background_noise\n",
    "\n",
    "    # Export de l'audio filtré\n",
    "    audio.export(output, format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation de la parole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_parole(audio_path, output_file, silence_threshold=-45):\n",
    "    # Charger le fichier audio\n",
    "    audio = AudioSegment.from_file(audio_path, format=\"wav\")\n",
    "\n",
    "    # Détection des silences\n",
    "    non_silent_audio = audio.strip_silence(silence_thresh=silence_threshold)\n",
    "\n",
    "    # Exporter le fichier audio sans les silences\n",
    "    non_silent_audio.export(output_file, format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Éliminé des artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_artifacts(audio_path, output_path):\n",
    "    # Chargement de l'enregistrement audio\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "    # Suppression d'artefacts basée sur la fréquence ou l'amplitude\n",
    "    # Par exemple, supprimer les fréquences inférieures à 1000 Hz\n",
    "    audio_filtered = audio.low_pass_filter(1000)\n",
    "\n",
    "    # Export de l'audio filtré\n",
    "    audio_filtered.export(output_path, format=\"wav\")\n",
    "# Exemple d'utilisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préaccentuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "def preaccentuation(audio_file, output):\n",
    "    # Charger le signal vocal (remplacer \"audio.wav\" par votre propre fichier audio)\n",
    "    sample_rate, audio_data = wavfile.read(audio_file)\n",
    "\n",
    "    # Paramètres de la préaccentuation\n",
    "    alpha = 0.95  # Facteur de préaccentuation (typiquement entre 0.9 et 1)\n",
    "\n",
    "    # Appliquer la préaccentuation\n",
    "    preemphasis_audio = np.append(audio_data[0], audio_data[1:] - alpha * audio_data[:-1])\n",
    "\n",
    "    # Enregistrer le signal filtré en tant que fichier WAV\n",
    "    wavfile.write(output, sample_rate, np.int16(preemphasis_audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation temporelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_stretch_audio(input_file, output_file, target_duration):\n",
    "    # Charger l'audio\n",
    "    audio, sr = librosa.load(input_file)\n",
    "\n",
    "    # Calculer la durée actuelle\n",
    "    current_duration = len(audio)/44100\n",
    "\n",
    "    # Calculer le facteur de normalisation\n",
    "    speed_factor = current_duration / target_duration\n",
    "\n",
    "    # Normaliser l'audio en modifiant la vitesse\n",
    "    normalized_audio = librosa.effects.time_stretch(y=audio, rate=speed_factor)\n",
    "\n",
    "    # Sauvegarder l'audio normalisé\n",
    "    sf.write(output_file, normalized_audio, sr)\n",
    "\n",
    "# Utilisation de la fonction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convertir en_format adapté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from torchvision.transforms import Compose\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "def convert_to_spectrogram(audio_path):\n",
    "    transform = Compose([\n",
    "        torchaudio.transforms.Resample(orig_freq=44100, new_freq=16000),  # Échantillonnage à 16 kHz\n",
    "        MelSpectrogram(n_fft=400, win_length=400, hop_length=160, n_mels=128)  # Créer un spectrogramme Mel\n",
    "    ])\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    spectrogram = transform(waveform).unsqueeze(0)  # Ajouter une dimension de lot\n",
    "    spectrogram = np.array(spectrogram)\n",
    "    spectrogram = spectrogram.reshape(-1)\n",
    "    \n",
    "    return spectrogram, sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction d'accent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_accent(audio_file):\n",
    "    signal, sample_rate = librosa.load(audio_file, sr=None)\n",
    "\n",
    "    # Calculer les coefficients cepstraux MFCC\n",
    "    mfccs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=13)\n",
    "\n",
    "    # Normaliser les coefficients MFCC\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(mfccs.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraitement audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "audio_dir = \"audio_test/\"\n",
    "# audio_dir = \"audio_chiffre/\"\n",
    "labels_train = []\n",
    "# Liste pour stocker les caractéristiques MFCC de chaque fichier audio\n",
    "mfcc_features_list = []\n",
    "# Chargement des labels\n",
    "with open(\"dataset/audio_path.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    transcriptions = json.load(f)\n",
    "\n",
    "for transcription in tqdm(transcriptions):\n",
    "# for filename in tqdm(os.listdir(audio_dir)):\n",
    "    # audio_path = os.path.join(audio_dir, filename)\n",
    "    audio_path = audio_dir+transcription[\"audio_file\"]\n",
    "    remove_silence(audio_path, \"remove_silence.wav\")\n",
    "    normalize_audio_volume(\"remove_silence.wav\", \"normalize_audio_volume.wav\")\n",
    "    time_stretch_audio(\"normalize_audio_volume.wav\", \"time_stretch_audio.wav\", target_length)\n",
    "    filtrage_du_bruit(\"time_stretch_audio.wav\", \"filtrage_du_bruit.wav\")\n",
    "    segmentation_parole(\"filtrage_du_bruit.wav\", \"segmentation_parole.wav\")\n",
    "    remove_artifacts(\"segmentation_parole.wav\", \"remove_artifacts.wav\")\n",
    "    preaccentuation(\"remove_artifacts.wav\", \"preaccentuation.wav\")\n",
    "    mfcc_features_normalized = correction_accent(\"preaccentuation.wav\")\n",
    "    mfcc_features_list.append(mfcc_features_normalized)\n",
    "    labels_train.append(transcription[\"transcription\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 13, 38)\n",
      "(2, 494)\n"
     ]
    }
   ],
   "source": [
    "# Convertir la liste en un tableau numpy\n",
    "X = np.array(mfcc_features_list)\n",
    "print(X.shape)\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "print(X.shape)\n",
    "# Enregistrement des caractéristiques prétraitées dans un fichier (optionnel)\n",
    "np.save(\"mfcc_feature.npy\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "# Convertir les labels en un format numérique (par exemple, utiliser un encodage one-hot)\n",
    "for i in range(len(labels_train)):\n",
    "    # Conversion en minuscules\n",
    "    labels_train[i] = labels_train[i].lower()\n",
    "    # Suppression de la ponctuation\n",
    "    labels_train[i] = re.sub(r'[^\\w\\s]', '', labels_train[i])\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# y = label_encoder.fit_transform(labels_train)\n",
    "# y = np.array(y)\n",
    "# print(y)\n",
    "vocab = []\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "for sentence in labels_train:\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        word_to_index[token] = len(word_to_index)-1\n",
    "        index_to_word[len(index_to_word)] = token\n",
    "        vocab.append(token)\n",
    "            \n",
    "print(vocab)\n",
    "# Encodage des séquences\n",
    "encoded_data = []\n",
    "for sentence in vocab:\n",
    "    tokens = sentence.split()\n",
    "    encoded_sentence = [word_to_index[token] for token in tokens]\n",
    "    encoded_data.append(encoded_sentence)\n",
    "encoded_data = np.array(encoded_data).reshape(-1)\n",
    "np.save(\"labels.npy\", encoded_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
